{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import gym\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "%matplotlib inline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import time\n",
    "\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Research\n",
    "The Lunar Lander Environment is a environment from the OpenAI Gym. The objective in this environment is to land a spaceship safely inbetween two flags, the space inbetween both flags will always be at the (0,0) coordinates.\n",
    "\n",
    "The environment has a state space with 8 attributes:\n",
    "- X-position\n",
    "- Y-position\n",
    "- X-velocity\n",
    "- Y-velocity\n",
    "- Angle of the Lander\n",
    "- Angular Velocity\n",
    "- If the left leg has touched the ground\n",
    "- If the right leg has touched the ground\n",
    "\n",
    "It also has an action space of 4 attributes:\n",
    "- Fire right engine\n",
    "- Fire left engine\n",
    "- Fire main engine\n",
    "- Do nothing\n",
    "\n",
    "The rewards for this environment are calculated by:\n",
    "- Moving from the top of the screen to the landing pad and coming to rest is about +100-140 points\n",
    "- If the lander moves away from the landing pad it loses reward\n",
    "- If the lander crashes it -100 points, if it lands it +100 points.\n",
    "- Each leg with ground contact is +10\n",
    "- Firing the main engine -0.3 points per frame (Fuel is infinite)\n",
    "- Firing the side engine -0.03 points per frame\n",
    "- Solved is 200 points\n",
    "\n",
    "Each episode will also automatically terminate if:\n",
    "- The lander crashes on contact with the moon\n",
    "- The lander gets outside of the viewport, where the x coordinate is greater than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Problem\n",
    "We were tasked to train a Reinforcement Learning algorithm to solve the LunarLanderV2 environment, by training it to land successfully on the landing pad.\n",
    "\n",
    "Reinforcement Learning is an area of machine learning where the objective is to train intelligent agents to take a sequence of actions in an environment to maximize the reward given to it. Therefore, to make the machine do what we want it to do, when the agent executes an action we will either reward it if its good. Or penalise it if the action is bad. This reward policy is already set for the LunarLander environment as seen from the background research I conducted above. And the goal is to train a Reinforcement Learning algorithm to take actions that will yield the most amount of rewards from the game, as that would mean that the algorithm is successful in landing the lander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Solution\n",
    "We will be exploring 2 Reinforcement Learning Algorithms, namely Deep Q Network for our baseline model and its improved counterpart Double Deep Q Network for our final model. We chose to explore these 2 models as we are able to compare the improvements of the DDQN model compared to the DQN model, and also talk about the difference that caused those improvements.\n",
    "\n",
    "Q Learning is a value based learning algorithm. These algorithms updates the value function based on an equation(particularly Bellman equation). Whereas the other type, policy-based estimates the value function with a greedy policy obtained from the last policy improvement. Q learning is a off-policy learner, meaning that it learns the optimal policy independently of the agent's action. An on-policy learner learns the value of the policy from the agent including exporation steps where it will find an optimal policy taking into account the exploration inherent in the policy.\n",
    "\n",
    "The Q table works by having a state action pair. As seen in this diagram:\n",
    "\n",
    "<img src='qtable.jpg'>\n",
    "\n",
    "From the diagram we can see that for each state there are multiple actions to be taken. For our problem, we would have 4 actions to be taken at any given state as described in our action space. Each state is consists of a different combination of observations values, which is taken from the environment to 'see' what state the lunar lander is in. From the background research, we know that there are 8 state space for the Lunar Lander environment. In a regular Q table, we would have to populate the table with Q values so as to figure out the optimal action to take at each given state. That would require enormous amount of computing power to figure out every possible state the lander is in and also which action to take in that state for optimal rewards.\n",
    "\n",
    "To speed up the process, we will use Deep Q Network. Where we train a neural network to predict what action to take based on a specific state that we input into it. Makes it so that we do not need to populate the whole Q table and find out the best action to take at every step as we are able to train a model to predict which action to take based on the values of the state you input into the neural network.\n",
    "\n",
    "We use Double Deep Q Network as our final model as its an improved architecture built to fix Deep Q Learning weaknesses, such as over estimation of the Q values as shown in [Deep Reinforcement Learning with Double Q-Learning](https://arxiv.org/pdf/1509.06461.pdf). The DDQN architechture not only reduces the over estimation seen in DQN, it is also reported to have better performance compared to DQN in several Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trace(x, y, ylabel, color):\n",
    "    trace = go.Scatter(\n",
    "        x=x, y=y, name=ylabel, marker=dict(color=color), mode=\"markers+lines\", text=x\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "def plot_graph(data):\n",
    "    avg = data[\"average\"]\n",
    "    reward = data[\"reward\"]\n",
    "    epochs = list(range(1, len(reward) + 1))\n",
    "\n",
    "    trace_avg = create_trace(epochs, avg, \"Avg Reward\", \"Green\")\n",
    "    trace_reward = create_trace(epochs, reward, \"Reward\", \"Red\")\n",
    "\n",
    "    fig = tools.make_subplots(\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        subplot_titles=(\n",
    "            \"Reward and Last 100 Average Reward\",\n",
    "        ),\n",
    "    )\n",
    "    fig.append_trace(trace_avg, 1, 1)\n",
    "    fig.append_trace(trace_reward, 1, 1)\n",
    "    fig[\"layout\"][\"xaxis\"].update(title=\"Epoch\")\n",
    "    fig.update_layout(height=800, width=2000)\n",
    "\n",
    "    iplot(fig, filename=\"accuracy-loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLogger():\n",
    "  def __init__(self, file_name='progress.log'):\n",
    "    self.file_name = file_name\n",
    "    self.clean_progress_file()\n",
    "\n",
    "  def log(self, episode, reward, average_reward):\n",
    "    f = open(self.file_name, 'a+')\n",
    "    f.write(f\"{episode};{reward};{average_reward}\\n\")\n",
    "    f.close()\n",
    "\n",
    "  def clean_progress_file(self):\n",
    "    if os.path.exists(self.file_name):\n",
    "      os.remove(self.file_name)\n",
    "    f = open(self.file_name, 'a+')\n",
    "    f.write(\"episode;reward;average\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "In a normal q-learning algorithm, the past experiences of the agent is not saved and used. The past experience of a model being defined as past states, actions, rewards and resultant states sets.\n",
    "\n",
    "Experience replay in DQN is when instead of throwing away these past experiences, we save them as history into memory so that we can revisit these sets of experiences for the model to be given a chance to update policy based on past state transitions.\n",
    "\n",
    "There are 2 main advantages of experience replay:\n",
    "\n",
    "1. Revisiting and learning on past experiences multiple times allows our model to learn more effieciently as policy updates are incremental (based on learning rate).\n",
    "\n",
    "2. Better convergence when training.\n",
    "\n",
    "In order to implement this, we create a Replay buffer class with attributes such as the memory size which specifies the max size our history.\n",
    "\n",
    "The way we store the sets of data is by using a numpy array containing numpy arrays of states, actions, rewards and resultant states.\n",
    "\n",
    "During every state in a episode, when we train our deep learning network we will use the sample buffer function to randomly select a batch of past experiences to to fed to our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.discrete = discrete\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        dtype = np.int8 if self.discrete else np.float32\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        # store one hot encoding of actions, if appropriate\n",
    "        if self.discrete:\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using huber loss when compiling our model. This is because when using mean squared error, the loss values can reach very high and extreme values which can cause the model to over calibrate.\n",
    "\n",
    "This will result in a higher failure of convergence. The huber loss is lesser prone to reach high and extreme values as it is only exponential up to a ceratin limit where by it will become linear. \n",
    "\n",
    "This reduces the chances of the model over calibrating which in turns leads to more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_huber_loss(mask_value, clip_delta):\n",
    "    def f(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) < clip_delta\n",
    "        mask_true = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "        masked_squared_error = 0.5 * K.square(mask_true * (y_true - y_pred))\n",
    "        linear_loss  = mask_true * (clip_delta * K.abs(error) - 0.5 * (clip_delta ** 2))\n",
    "        huber_loss = tf.where(cond, masked_squared_error, linear_loss)\n",
    "        return K.sum(huber_loss) / K.sum(mask_true)\n",
    "    f.__name__ = 'masked_huber_loss'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a simple fully connected network which will contain 2 dense layers and 2 relu activation layers between them.\n",
    "\n",
    "We will have an output dense layer with 4 nodes each representing an action the agent is able to take. \n",
    "\n",
    "We will be using the adam optimiser and the huber loss for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dqn(lr, n_actions, input_dims):\n",
    "    model = Sequential([\n",
    "                Dense(256, input_shape=(input_dims,)),\n",
    "                Activation('relu'),\n",
    "                Dense(256),\n",
    "                Activation('relu'),\n",
    "                Dense(n_actions)])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=masked_huber_loss(0,1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build our agent. Our agent comprises of all the hyperparameters needed to optimize the policy as well as the replay buffer and deep learning network.\n",
    "\n",
    "The remember function in the agent is for us to store the states, actions, rewards and resultant states sets into the history of the ReplayBuffer class.\n",
    "\n",
    "The learn function is where we call the ReplayBuffer class to randomly sample 64 states, actions, rewards and resultant states sets from the experience history to use it to calculate the the new q values for the states using the bellman's equation. The Bellman equation looks like this:\n",
    "\n",
    "<image src = 'https://cdn.discordapp.com/attachments/345195124374110218/944283494501187724/image-1.png'>\n",
    "\n",
    "In Q learning an agent uses the Bellman equation to update the Q values of state action pairs. We used this formula to get and fit the new q values of the 64 states that we received from the replaybuffer class to our deep learning network model.\n",
    "\n",
    "We also update the epsilon value over here which is the exploration vs exploitation rate in the learn function.\n",
    "\n",
    "The last 2 functions are save_model to save the current deep learning model and load_model is for loading a deeping learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,\n",
    "                input_dims, epsilon_dec, epsilon_min,\n",
    "                mem_size):\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions, discrete=True)\n",
    "        self.q_eval = get_dqn(alpha, n_actions, input_dims)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr > self.batch_size:\n",
    "            state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "            action_values = np.array(self.action_space, dtype=np.int8)\n",
    "            action_indices = np.dot(action, action_values)\n",
    "\n",
    "            q_eval = self.q_eval.predict(state)\n",
    "\n",
    "            q_next = self.q_eval.predict(new_state)\n",
    "\n",
    "            q_target = q_eval.copy()\n",
    "\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "            q_target[batch_index, action_indices] = reward + self.gamma*np.max(q_next, axis=1)*done\n",
    "\n",
    "            _ = self.q_eval.fit(state, q_target, verbose=0)\n",
    "\n",
    "            self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        self.q_eval.save(model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main program we will be specifying the hyper parameters and also creating the main loop where we choose the number of episodes/games to run.\n",
    "\n",
    "The chosen values for all the hyperparameters are based on experimention\n",
    "\n",
    "The hyper parameters are:\n",
    "\n",
    "1. Learning rate: The learning rate is the leaning rate used for the adam optimizer for the deep learning network. The lower the learning rate the smaller the changes to the weights of the models are modified and vice versa. We will be using a learning rate of 0.0005.\n",
    "<br><br>\n",
    "\n",
    "2. Gamma: Gamma is the discount factor used in the bellman equation. The higher the gamma, the more the agent will value long term rewards. We will be using a gamma of 0.99.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "3. Epsilon: The epsilon is the probability at which we either use our model to choose the action (explotation) or use randomly choose a action (exploration). An epsilon of 1 indicates only using random actions while an epsilon of 0 indicates only using the model to predict. Our staring epsilon will be 1 as we want to explore the environment as much possible at the start.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "4. Epsilon decay: The epsilon is the rate at which the epsilon is reduced as the number of episodes increases. This is so that we start exploiting our policy more as we train the model longer. We wil be using a decay rate of 0.996.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "5. input dimensions: The input dimension represents the number of observations in each state. The lunar lander environment has 8 observations so we will be using 8 as our input dimension.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "6. Number of actions: The number of actions represents the number of actions our agent can take. The lunar lander environment has 4 actions so we will be using 4 as our input dimension.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "7. memory size: The memory size represents the max number of entrys for the history of states, actions, rewards and resultant states sets. When the number of sets exceed this number the first item of the dataset will be removed to make space for he new entry. We will be using a memory size of 1000000\n",
    "<br><br>\n",
    "\n",
    "\n",
    "8. batch size: The batch size represents the number of states, actions, rewards and resultant states sets sampled from the history when the agent is learning. We will be using 64\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Inside the main loop, we will have a while loop to continuously take actions and train the model until the terminal state is reached. We then repeat n_games times which in this case is 500 times. After every games, we print out the score and average score. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(alpha=0.0005, gamma=0.99, n_actions=4, epsilon=1, batch_size=64, input_dims=8, epsilon_dec=0.995, epsilon_min=0.01, mem_size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andric\\anaconda3\\envs\\Reinforcement\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: -263.75  average score -263.75\n",
      "episode:  1 score: -462.09  average score -362.92\n",
      "episode:  2 score: -107.21  average score -277.68\n",
      "episode:  3 score: -399.87  average score -308.23\n",
      "episode:  4 score: -393.01  average score -325.19\n",
      "episode:  5 score: -220.47  average score -307.73\n",
      "episode:  6 score: -262.93  average score -301.33\n",
      "episode:  7 score: -46.14  average score -269.43\n",
      "episode:  8 score: -173.41  average score -258.77\n",
      "episode:  9 score: -128.90  average score -245.78\n",
      "episode:  10 score: 97.87  average score -214.54\n",
      "episode:  11 score: -229.66  average score -215.80\n",
      "episode:  12 score: -97.91  average score -206.73\n",
      "episode:  13 score: -377.40  average score -218.92\n",
      "episode:  14 score: -164.32  average score -215.28\n",
      "episode:  15 score: -185.68  average score -213.43\n",
      "episode:  16 score: -94.54  average score -206.44\n",
      "episode:  17 score: -195.71  average score -205.84\n",
      "episode:  18 score: -77.49  average score -199.09\n",
      "episode:  19 score: -97.40  average score -194.00\n",
      "episode:  20 score: -270.91  average score -197.66\n",
      "episode:  21 score: -123.39  average score -194.29\n",
      "episode:  22 score: -142.63  average score -192.04\n",
      "episode:  23 score: -83.47  average score -187.52\n",
      "episode:  24 score: -59.32  average score -182.39\n",
      "episode:  25 score: -98.27  average score -179.15\n",
      "episode:  26 score: -180.67  average score -179.21\n",
      "episode:  27 score: -97.22  average score -176.28\n",
      "episode:  28 score: -22.84  average score -170.99\n",
      "episode:  29 score: -115.85  average score -169.15\n",
      "episode:  30 score: -98.89  average score -166.89\n",
      "episode:  31 score: -62.18  average score -163.61\n",
      "episode:  32 score: -142.81  average score -162.98\n",
      "episode:  33 score: -74.14  average score -160.37\n",
      "episode:  34 score: -25.41  average score -156.51\n",
      "episode:  35 score: -67.80  average score -154.05\n",
      "episode:  36 score: -62.94  average score -151.59\n",
      "episode:  37 score: -59.34  average score -149.16\n",
      "episode:  38 score: -46.32  average score -146.52\n",
      "episode:  39 score: -657.07  average score -159.29\n",
      "episode:  40 score: -52.39  average score -156.68\n",
      "episode:  41 score: -224.83  average score -158.30\n",
      "episode:  42 score: -52.64  average score -155.85\n",
      "episode:  43 score: -167.68  average score -156.11\n",
      "episode:  44 score: -224.59  average score -157.64\n",
      "episode:  45 score: -294.08  average score -160.60\n",
      "episode:  46 score: -347.85  average score -164.59\n",
      "episode:  47 score: -237.16  average score -166.10\n",
      "episode:  48 score: -256.83  average score -167.95\n",
      "episode:  49 score: -87.99  average score -166.35\n",
      "episode:  50 score: -212.58  average score -167.26\n",
      "episode:  51 score: -167.70  average score -167.27\n",
      "episode:  52 score: -132.89  average score -166.62\n",
      "episode:  53 score: -36.80  average score -164.21\n",
      "episode:  54 score: -33.16  average score -161.83\n",
      "episode:  55 score: -33.21  average score -159.53\n",
      "episode:  56 score: -71.45  average score -157.99\n",
      "episode:  57 score: -147.74  average score -157.81\n",
      "episode:  58 score: -200.93  average score -158.54\n",
      "episode:  59 score: -249.22  average score -160.05\n",
      "episode:  60 score: -26.95  average score -157.87\n",
      "episode:  61 score: -62.62  average score -156.34\n",
      "episode:  62 score: -46.54  average score -154.59\n",
      "episode:  63 score: -38.63  average score -152.78\n",
      "episode:  64 score: -58.83  average score -151.34\n",
      "episode:  65 score: -89.08  average score -150.39\n",
      "episode:  66 score: -50.70  average score -148.90\n",
      "episode:  67 score: -66.51  average score -147.69\n",
      "episode:  68 score: -147.51  average score -147.69\n",
      "episode:  69 score: -63.84  average score -146.49\n",
      "episode:  70 score: -69.37  average score -145.41\n",
      "episode:  71 score: -108.44  average score -144.89\n",
      "episode:  72 score: -108.30  average score -144.39\n",
      "episode:  73 score: -73.79  average score -143.44\n",
      "episode:  74 score: -55.98  average score -142.27\n",
      "episode:  75 score: -48.14  average score -141.03\n",
      "episode:  76 score: -22.04  average score -139.49\n",
      "episode:  77 score: -49.04  average score -138.33\n",
      "episode:  78 score: -76.77  average score -137.55\n",
      "episode:  79 score: -127.72  average score -137.43\n",
      "episode:  80 score: -99.41  average score -136.96\n",
      "episode:  81 score: -54.79  average score -135.95\n",
      "episode:  82 score: -31.20  average score -134.69\n",
      "episode:  83 score: -57.02  average score -133.77\n",
      "episode:  84 score: -67.69  average score -132.99\n",
      "episode:  85 score: -79.22  average score -132.36\n",
      "episode:  86 score: -25.67  average score -131.14\n",
      "episode:  87 score: -5.38  average score -129.71\n",
      "episode:  88 score: -31.23  average score -128.60\n",
      "episode:  89 score: -51.14  average score -127.74\n",
      "episode:  90 score: -60.37  average score -127.00\n",
      "episode:  91 score: -81.22  average score -126.50\n",
      "episode:  92 score: -58.74  average score -125.78\n",
      "episode:  93 score: -53.43  average score -125.01\n",
      "episode:  94 score: -244.35  average score -126.26\n",
      "episode:  95 score: -3.44  average score -124.98\n",
      "episode:  96 score: -35.98  average score -124.07\n",
      "episode:  97 score: -7.92  average score -122.88\n",
      "episode:  98 score: -77.42  average score -122.42\n",
      "episode:  99 score: -358.36  average score -124.78\n",
      "episode:  100 score: -8.14  average score -123.63\n",
      "episode:  101 score: 195.84  average score -119.07\n",
      "episode:  102 score: 173.59  average score -112.78\n",
      "episode:  103 score: 29.23  average score -111.43\n",
      "episode:  104 score: 3.10  average score -107.44\n",
      "episode:  105 score: -66.79  average score -104.21\n",
      "episode:  106 score: -32.81  average score -102.35\n",
      "episode:  107 score: -30.37  average score -100.05\n",
      "episode:  108 score: -50.85  average score -100.10\n",
      "episode:  109 score: -61.85  average score -98.99\n",
      "episode:  110 score: 153.70  average score -96.19\n",
      "episode:  111 score: -16.27  average score -97.32\n",
      "episode:  112 score: -11.33  average score -95.16\n",
      "episode:  113 score: 7.13  average score -94.12\n",
      "episode:  114 score: -37.09  average score -90.75\n",
      "episode:  115 score: -57.87  average score -89.70\n",
      "episode:  116 score: 20.43  average score -87.66\n",
      "episode:  117 score: 219.26  average score -84.55\n",
      "episode:  118 score: 239.19  average score -80.25\n",
      "episode:  119 score: 120.85  average score -78.28\n",
      "episode:  120 score: 98.64  average score -76.34\n",
      "episode:  121 score: -40.90  average score -74.06\n",
      "episode:  122 score: 54.29  average score -72.30\n",
      "episode:  123 score: -30.39  average score -71.19\n",
      "episode:  124 score: -394.41  average score -74.27\n",
      "episode:  125 score: -324.20  average score -76.89\n",
      "episode:  126 score: -139.45  average score -77.30\n",
      "episode:  127 score: 56.31  average score -74.96\n",
      "episode:  128 score: 46.52  average score -73.53\n",
      "episode:  129 score: 238.05  average score -70.95\n",
      "episode:  130 score: 190.85  average score -67.91\n",
      "episode:  131 score: -50.15  average score -67.43\n",
      "episode:  132 score: 215.86  average score -64.68\n",
      "episode:  133 score: -24.32  average score -63.50\n",
      "episode:  134 score: 225.53  average score -60.54\n",
      "episode:  135 score: 172.14  average score -58.58\n",
      "episode:  136 score: 22.17  average score -57.69\n",
      "episode:  137 score: 110.47  average score -55.97\n",
      "episode:  138 score: 157.38  average score -53.83\n",
      "episode:  139 score: 147.27  average score -51.91\n",
      "episode:  140 score: 167.35  average score -43.75\n",
      "episode:  141 score: 242.79  average score -40.83\n",
      "episode:  142 score: 221.93  average score -36.40\n",
      "episode:  143 score: 43.30  average score -35.45\n",
      "episode:  144 score: 25.23  average score -33.54\n",
      "episode:  145 score: 226.43  average score -29.08\n",
      "episode:  146 score: 218.74  average score -24.00\n",
      "episode:  147 score: 2.72  average score -20.53\n",
      "episode:  148 score: 205.94  average score -16.14\n",
      "episode:  149 score: 206.83  average score -11.55\n",
      "episode:  150 score: 193.93  average score -8.76\n",
      "episode:  151 score: 54.09  average score -6.12\n",
      "episode:  152 score: 218.88  average score -2.29\n",
      "episode:  153 score: 169.08  average score 0.70\n",
      "episode:  154 score: -19.16  average score 0.87\n",
      "episode:  155 score: 248.68  average score 3.66\n",
      "episode:  156 score: 18.29  average score 4.17\n",
      "episode:  157 score: -2.25  average score 4.86\n",
      "episode:  158 score: -62.17  average score 5.71\n",
      "episode:  159 score: 176.15  average score 9.44\n",
      "episode:  160 score: -47.09  average score 11.44\n",
      "episode:  161 score: 138.03  average score 13.07\n",
      "episode:  162 score: 226.33  average score 15.94\n",
      "episode:  163 score: 129.37  average score 17.68\n",
      "episode:  164 score: 204.95  average score 20.09\n",
      "episode:  165 score: 227.36  average score 22.92\n",
      "episode:  166 score: 248.03  average score 26.26\n",
      "episode:  167 score: 265.60  average score 29.39\n",
      "episode:  168 score: 251.50  average score 32.54\n",
      "episode:  169 score: -622.69  average score 27.84\n",
      "episode:  170 score: 162.84  average score 30.08\n",
      "episode:  171 score: 207.94  average score 32.83\n",
      "episode:  172 score: 271.77  average score 36.59\n",
      "episode:  173 score: 208.22  average score 39.72\n",
      "episode:  174 score: 217.67  average score 42.61\n",
      "episode:  175 score: 273.57  average score 45.87\n",
      "episode:  176 score: 229.40  average score 48.62\n",
      "episode:  177 score: 239.81  average score 51.21\n",
      "episode:  178 score: 259.85  average score 54.27\n",
      "episode:  179 score: 224.17  average score 57.25\n",
      "episode:  180 score: 77.76  average score 59.29\n",
      "episode:  181 score: 36.48  average score 60.63\n",
      "episode:  182 score: 206.06  average score 63.21\n",
      "episode:  183 score: 252.41  average score 66.02\n",
      "episode:  184 score: 120.76  average score 67.78\n",
      "episode:  185 score: 208.30  average score 70.51\n",
      "episode:  186 score: 203.02  average score 73.31\n",
      "episode:  187 score: 259.21  average score 76.13\n",
      "episode:  188 score: 97.66  average score 77.15\n",
      "episode:  189 score: 93.65  average score 78.39\n",
      "episode:  190 score: 130.17  average score 80.18\n",
      "episode:  191 score: 219.78  average score 82.95\n",
      "episode:  192 score: 241.96  average score 86.15\n",
      "episode:  193 score: 299.26  average score 89.70\n",
      "episode:  194 score: 147.52  average score 91.69\n",
      "episode:  195 score: 215.68  average score 96.24\n",
      "episode:  196 score: 206.33  average score 98.32\n",
      "episode:  197 score: 206.64  average score 100.72\n",
      "episode:  198 score: 227.75  average score 103.06\n",
      "episode:  199 score: 235.70  average score 106.16\n",
      "episode:  200 score: 241.45  average score 112.09\n",
      "episode:  201 score: 263.47  average score 114.78\n",
      "episode:  202 score: 57.26  average score 113.41\n",
      "episode:  203 score: 278.32  average score 114.45\n",
      "episode:  204 score: 224.78  average score 116.39\n",
      "episode:  205 score: 59.00  average score 116.94\n",
      "episode:  206 score: 254.19  average score 120.12\n",
      "episode:  207 score: -53.87  average score 119.91\n",
      "episode:  208 score: 232.28  average score 122.51\n",
      "episode:  209 score: 275.68  average score 125.74\n",
      "episode:  210 score: 172.30  average score 128.06\n",
      "episode:  211 score: 231.84  average score 128.83\n",
      "episode:  212 score: -28.49  average score 128.71\n",
      "episode:  213 score: -214.31  average score 126.70\n",
      "episode:  214 score: -230.73  average score 124.35\n",
      "episode:  215 score: 243.84  average score 127.13\n",
      "episode:  216 score: 209.23  average score 129.77\n",
      "episode:  217 score: -23.40  average score 129.34\n",
      "episode:  218 score: -329.03  average score 123.91\n",
      "episode:  219 score: 246.74  average score 123.99\n",
      "episode:  220 score: -3.48  average score 122.75\n",
      "episode:  221 score: 238.49  average score 124.14\n",
      "episode:  222 score: 57.84  average score 125.12\n",
      "episode:  223 score: 167.42  average score 126.24\n",
      "episode:  224 score: 244.76  average score 128.96\n",
      "episode:  225 score: 4.18  average score 132.91\n",
      "episode:  226 score: -24.90  average score 135.87\n",
      "episode:  227 score: 216.52  average score 139.40\n",
      "episode:  228 score: 184.48  average score 140.66\n",
      "episode:  229 score: 269.87  average score 142.88\n",
      "episode:  230 score: 176.24  average score 142.26\n",
      "episode:  231 score: -76.04  average score 139.62\n",
      "episode:  232 score: 22.71  average score 140.34\n",
      "episode:  233 score: 211.82  average score 140.30\n",
      "episode:  234 score: 241.30  average score 142.93\n",
      "episode:  235 score: 228.15  average score 142.96\n",
      "episode:  236 score: 241.87  average score 143.65\n",
      "episode:  237 score: 185.44  average score 145.27\n",
      "episode:  238 score: -45.99  average score 143.72\n",
      "episode:  239 score: -3.91  average score 142.12\n",
      "episode:  240 score: 254.31  average score 143.18\n",
      "episode:  241 score: 242.31  average score 143.92\n",
      "episode:  242 score: 26.93  average score 141.78\n",
      "episode:  243 score: 103.90  average score 140.62\n",
      "episode:  244 score: 177.95  average score 141.95\n",
      "episode:  245 score: 54.51  average score 142.24\n",
      "episode:  246 score: 5.39  average score 140.05\n",
      "episode:  247 score: 204.96  average score 139.91\n",
      "episode:  248 score: 279.59  average score 142.66\n",
      "episode:  249 score: 248.97  average score 143.08\n",
      "episode:  250 score: 220.34  average score 143.22\n",
      "episode:  251 score: 177.24  average score 143.05\n",
      "episode:  252 score: 163.42  average score 144.13\n",
      "episode:  253 score: 93.63  average score 142.89\n",
      "episode:  254 score: 239.29  average score 143.59\n",
      "episode:  255 score: 257.29  average score 146.32\n",
      "episode:  256 score: 242.44  average score 146.26\n",
      "episode:  257 score: 237.22  average score 148.43\n",
      "episode:  258 score: 212.75  average score 150.56\n",
      "episode:  259 score: 140.62  average score 152.57\n",
      "episode:  260 score: 245.84  average score 153.26\n",
      "episode:  261 score: 197.51  average score 155.68\n",
      "episode:  262 score: 225.10  average score 156.54\n",
      "episode:  263 score: 256.54  average score 156.84\n",
      "episode:  264 score: -48.33  average score 155.08\n",
      "episode:  265 score: 4.20  average score 153.09\n",
      "episode:  266 score: -42.78  average score 150.42\n",
      "episode:  267 score: 161.62  average score 149.56\n",
      "episode:  268 score: 235.16  average score 149.26\n",
      "episode:  269 score: 215.59  average score 148.91\n",
      "episode:  270 score: 252.28  average score 157.57\n",
      "episode:  271 score: -189.53  average score 154.08\n",
      "episode:  272 score: 90.79  average score 152.92\n",
      "episode:  273 score: -40.03  average score 149.83\n",
      "episode:  274 score: 177.37  average score 149.53\n",
      "episode:  275 score: 31.97  average score 147.69\n",
      "episode:  276 score: 18.29  average score 145.16\n",
      "episode:  277 score: 45.74  average score 143.34\n",
      "episode:  278 score: -12.74  average score 140.84\n",
      "episode:  279 score: 292.97  average score 141.17\n",
      "episode:  280 score: 163.79  average score 140.57\n",
      "episode:  281 score: 216.59  average score 141.95\n",
      "episode:  282 score: 241.88  average score 143.98\n",
      "episode:  283 score: -56.53  average score 141.38\n",
      "episode:  284 score: -34.94  average score 138.54\n",
      "episode:  285 score: -64.73  average score 136.70\n",
      "episode:  286 score: 263.95  average score 137.25\n",
      "episode:  287 score: 101.84  average score 136.25\n",
      "episode:  288 score: 211.15  average score 135.77\n",
      "episode:  289 score: -62.11  average score 134.19\n",
      "episode:  290 score: 211.22  average score 135.36\n",
      "episode:  291 score: 255.88  average score 136.60\n",
      "episode:  292 score: 202.82  average score 136.43\n",
      "episode:  293 score: 290.22  average score 136.91\n",
      "episode:  294 score: 234.80  average score 136.27\n",
      "episode:  295 score: 196.59  average score 136.76\n",
      "episode:  296 score: 215.18  average score 136.75\n",
      "episode:  297 score: 257.12  average score 137.26\n",
      "episode:  298 score: -104.19  average score 134.18\n",
      "episode:  299 score: 60.52  average score 132.52\n",
      "episode:  300 score: -81.48  average score 129.38\n",
      "episode:  301 score: -9.87  average score 126.89\n",
      "episode:  302 score: -69.11  average score 123.60\n",
      "episode:  303 score: 234.84  average score 125.36\n",
      "episode:  304 score: 10.02  average score 122.70\n",
      "episode:  305 score: -8.52  average score 120.39\n",
      "episode:  306 score: 248.64  average score 122.27\n",
      "episode:  307 score: 247.06  average score 122.20\n",
      "episode:  308 score: -53.14  average score 122.21\n",
      "episode:  309 score: -19.60  average score 119.71\n",
      "episode:  310 score: 286.77  average score 119.82\n",
      "episode:  311 score: 269.15  average score 120.78\n",
      "episode:  312 score: 296.19  average score 121.42\n",
      "episode:  313 score: 247.23  average score 124.15\n",
      "episode:  314 score: 49.18  average score 126.76\n",
      "episode:  315 score: 69.72  average score 129.73\n",
      "episode:  316 score: -25.09  average score 127.07\n",
      "episode:  317 score: 49.20  average score 125.48\n",
      "episode:  318 score: 198.72  average score 127.68\n",
      "episode:  319 score: 269.28  average score 133.61\n",
      "episode:  320 score: 265.80  average score 133.80\n",
      "episode:  321 score: 264.19  average score 136.45\n",
      "episode:  322 score: 280.12  average score 136.86\n",
      "episode:  323 score: 246.89  average score 138.73\n",
      "episode:  324 score: -3.02  average score 137.04\n",
      "episode:  325 score: 24.06  average score 134.86\n",
      "episode:  326 score: 288.67  average score 137.67\n",
      "episode:  327 score: 277.23  average score 140.67\n",
      "episode:  328 score: 226.93  average score 140.77\n",
      "episode:  329 score: 292.16  average score 141.84\n",
      "episode:  330 score: 28.87  average score 139.45\n",
      "episode:  331 score: 276.42  average score 140.44\n",
      "episode:  332 score: 233.93  average score 143.51\n",
      "episode:  333 score: 230.17  average score 145.56\n",
      "episode:  334 score: 242.22  average score 145.87\n",
      "episode:  335 score: 246.62  average score 145.92\n",
      "episode:  336 score: 267.51  average score 146.31\n",
      "episode:  337 score: 276.62  average score 146.65\n",
      "episode:  338 score: 258.60  average score 147.38\n",
      "episode:  339 score: 246.05  average score 150.27\n",
      "episode:  340 score: 247.49  average score 152.76\n",
      "episode:  341 score: 34.60  average score 150.58\n",
      "episode:  342 score: 272.67  average score 150.88\n",
      "episode:  343 score: 262.04  average score 153.21\n",
      "episode:  344 score: 276.26  average score 154.92\n",
      "episode:  345 score: 205.44  average score 155.19\n",
      "episode:  346 score: 15.77  average score 154.81\n",
      "episode:  347 score: 250.32  average score 157.23\n",
      "episode:  348 score: 249.50  average score 157.67\n",
      "episode:  349 score: 246.40  average score 157.34\n",
      "episode:  350 score: 262.14  average score 157.47\n",
      "episode:  351 score: 262.88  average score 157.89\n",
      "episode:  352 score: 283.20  average score 158.94\n",
      "episode:  353 score: 265.18  average score 159.95\n",
      "episode:  354 score: 96.87  average score 159.98\n",
      "episode:  355 score: 259.60  average score 160.18\n",
      "episode:  356 score: 257.53  average score 160.19\n",
      "episode:  357 score: 231.64  average score 160.08\n",
      "episode:  358 score: 245.27  average score 160.16\n",
      "episode:  359 score: -26.68  average score 157.79\n",
      "episode:  360 score: 290.12  average score 159.27\n",
      "episode:  361 score: 157.47  average score 158.39\n",
      "episode:  362 score: 251.30  average score 158.93\n",
      "episode:  363 score: 217.63  average score 158.85\n",
      "episode:  364 score: 237.82  average score 158.67\n",
      "episode:  365 score: -29.68  average score 158.85\n",
      "episode:  366 score: 13.00  average score 158.94\n",
      "episode:  367 score: 243.08  average score 161.77\n",
      "episode:  368 score: 49.31  average score 160.66\n",
      "episode:  369 score: 204.99  average score 160.36\n",
      "episode:  370 score: -252.83  average score 155.72\n",
      "episode:  371 score: -155.11  average score 151.69\n",
      "episode:  372 score: -322.64  average score 150.37\n",
      "episode:  373 score: -267.84  average score 146.82\n",
      "episode:  374 score: -298.74  average score 144.26\n",
      "episode:  375 score: -317.66  average score 139.36\n",
      "episode:  376 score: -350.62  average score 135.57\n",
      "episode:  377 score: -254.14  average score 132.87\n",
      "episode:  378 score: -181.84  average score 130.62\n",
      "episode:  379 score: -147.67  average score 129.28\n",
      "episode:  380 score: -17.33  average score 126.21\n",
      "episode:  381 score: -219.66  average score 122.41\n",
      "episode:  382 score: 176.22  average score 122.01\n",
      "episode:  383 score: -2.01  average score 119.60\n",
      "episode:  384 score: 8.20  average score 120.24\n",
      "episode:  385 score: 212.37  average score 122.69\n",
      "episode:  386 score: -78.85  average score 122.55\n",
      "episode:  387 score: 5.06  average score 119.98\n",
      "episode:  388 score: 25.41  average score 119.23\n",
      "episode:  389 score: -86.96  average score 116.28\n",
      "episode:  390 score: 255.45  average score 119.42\n",
      "episode:  391 score: -35.60  average score 116.98\n",
      "episode:  392 score: 243.84  average score 116.86\n",
      "episode:  393 score: -73.86  average score 114.12\n",
      "episode:  394 score: -72.77  average score 110.52\n",
      "episode:  395 score: -133.96  average score 106.87\n",
      "episode:  396 score: -251.79  average score 102.43\n",
      "episode:  397 score: -253.65  average score 97.79\n",
      "episode:  398 score: -220.71  average score 93.06\n",
      "episode:  399 score: -146.12  average score 92.64\n",
      "episode:  400 score: -84.01  average score 91.21\n",
      "episode:  401 score: -94.24  average score 91.09\n",
      "episode:  402 score: 237.87  average score 93.54\n",
      "episode:  403 score: 260.77  average score 96.81\n",
      "episode:  404 score: 213.09  average score 96.59\n",
      "episode:  405 score: 209.18  average score 98.56\n",
      "episode:  406 score: 16.05  average score 98.81\n",
      "episode:  407 score: 238.50  average score 98.71\n",
      "episode:  408 score: 23.07  average score 96.49\n",
      "episode:  409 score: -66.51  average score 96.36\n",
      "episode:  410 score: 240.83  average score 98.93\n",
      "episode:  411 score: 254.03  average score 98.61\n",
      "episode:  412 score: -232.89  average score 93.64\n",
      "episode:  413 score: 171.16  average score 92.40\n",
      "episode:  414 score: 284.02  average score 92.77\n",
      "episode:  415 score: 211.78  average score 94.38\n",
      "episode:  416 score: 237.75  average score 96.04\n",
      "episode:  417 score: 239.55  average score 98.66\n",
      "episode:  418 score: 212.11  average score 100.27\n",
      "episode:  419 score: 119.67  average score 99.49\n",
      "episode:  420 score: 46.17  average score 97.28\n",
      "episode:  421 score: 245.25  average score 97.08\n",
      "episode:  422 score: 261.35  average score 97.05\n",
      "episode:  423 score: 267.32  average score 96.92\n",
      "episode:  424 score: -1.70  average score 94.46\n",
      "episode:  425 score: 209.49  average score 96.57\n",
      "episode:  426 score: 259.69  average score 98.90\n",
      "episode:  427 score: 252.48  average score 98.54\n",
      "episode:  428 score: 138.60  average score 97.17\n",
      "episode:  429 score: 209.47  average score 96.99\n",
      "episode:  430 score: -0.38  average score 94.10\n",
      "episode:  431 score: 263.87  average score 96.42\n",
      "episode:  432 score: 4.30  average score 93.73\n",
      "episode:  433 score: 215.08  average score 93.54\n",
      "episode:  434 score: 276.13  average score 94.00\n",
      "episode:  435 score: -19.15  average score 91.41\n",
      "episode:  436 score: 227.08  average score 91.22\n",
      "episode:  437 score: 188.13  average score 90.43\n",
      "episode:  438 score: 166.45  average score 89.34\n",
      "episode:  439 score: 49.82  average score 87.27\n",
      "episode:  440 score: 26.32  average score 85.10\n",
      "episode:  441 score: -0.26  average score 82.65\n",
      "episode:  442 score: 239.55  average score 84.67\n",
      "episode:  443 score: 259.07  average score 84.54\n",
      "episode:  444 score: -37.66  average score 81.57\n",
      "episode:  445 score: 3.43  average score 78.87\n",
      "episode:  446 score: 275.01  average score 79.56\n",
      "episode:  447 score: 235.82  average score 81.74\n",
      "episode:  448 score: 213.50  average score 81.37\n",
      "episode:  449 score: 110.64  average score 80.00\n",
      "episode:  450 score: 280.47  average score 80.34\n",
      "episode:  451 score: 259.44  average score 80.31\n",
      "episode:  452 score: 53.33  average score 78.23\n",
      "episode:  453 score: 247.75  average score 77.88\n",
      "episode:  454 score: 271.39  average score 77.95\n",
      "episode:  455 score: 266.81  average score 79.63\n",
      "episode:  456 score: 47.50  average score 77.53\n",
      "episode:  457 score: 110.76  average score 76.07\n",
      "episode:  458 score: 51.27  average score 74.29\n",
      "episode:  459 score: 46.19  average score 72.32\n",
      "episode:  460 score: 281.90  average score 75.37\n",
      "episode:  461 score: 286.54  average score 75.34\n",
      "episode:  462 score: 156.56  average score 75.33\n",
      "episode:  463 score: 270.60  average score 75.52\n",
      "episode:  464 score: 215.89  average score 75.50\n",
      "episode:  465 score: 258.49  average score 75.71\n",
      "episode:  466 score: 278.39  average score 78.76\n",
      "episode:  467 score: -69.15  average score 77.94\n",
      "episode:  468 score: 40.13  average score 75.93\n",
      "episode:  469 score: -137.48  average score 74.09\n",
      "episode:  470 score: 101.82  average score 73.06\n",
      "episode:  471 score: 212.80  average score 77.67\n",
      "episode:  472 score: 261.44  average score 81.80\n",
      "episode:  473 score: 80.91  average score 85.79\n",
      "episode:  474 score: 241.80  average score 90.84\n",
      "episode:  475 score: 260.32  average score 96.37\n",
      "episode:  476 score: 253.51  average score 102.03\n",
      "episode:  477 score: 265.02  average score 108.13\n",
      "episode:  478 score: 233.99  average score 112.96\n",
      "episode:  479 score: 57.62  average score 115.33\n",
      "episode:  480 score: 239.58  average score 119.16\n",
      "episode:  481 score: -19.30  average score 119.14\n",
      "episode:  482 score: 64.53  average score 121.96\n",
      "episode:  483 score: 270.58  average score 122.89\n",
      "episode:  484 score: 299.10  average score 125.87\n",
      "episode:  485 score: 13.76  average score 125.93\n",
      "episode:  486 score: 261.94  average score 126.42\n",
      "episode:  487 score: 21.60  average score 127.41\n",
      "episode:  488 score: 283.45  average score 130.17\n",
      "episode:  489 score: 70.80  average score 130.62\n",
      "episode:  490 score: -5.78  average score 131.42\n",
      "episode:  491 score: 253.18  average score 131.40\n",
      "episode:  492 score: 265.42  average score 134.38\n",
      "episode:  493 score: 264.95  average score 134.59\n",
      "episode:  494 score: 271.05  average score 138.01\n",
      "episode:  495 score: 257.55  average score 141.28\n",
      "episode:  496 score: 278.44  average score 145.36\n",
      "episode:  497 score: 275.82  average score 150.58\n",
      "episode:  498 score: 257.78  average score 155.65\n",
      "episode:  499 score: 56.79  average score 158.39\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "reward_history = []\n",
    "eps_history = []\n",
    "logger = FileLogger('DQN_History.log')\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.remember(observation, action, reward, observation_, int(done))\n",
    "        observation = observation_\n",
    "        agent.learn()\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n",
    "    reward_history.append(score)\n",
    "\n",
    "    avg_score = np.mean(reward_history[max(0, i-100):(i+1)])\n",
    "    print('episode: ', i,'score: %.2f' % score,\n",
    "            ' average score %.2f' % avg_score)\n",
    "\n",
    "    if i % 5 == 0 and i > 0:\n",
    "        agent.save_model(f'Models/DQN/episode{i}.h5')\n",
    "    logger.log(i, score, avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating DQN\n",
    "\n",
    "From the graph below, we can see the moving average of 100 for rewards in green. And also the individual rewards per episode in red. We can see that the moving average increases drastically from episodes 0 to 200. Then it starts to slow down its increments and peaks at episode 271 before it dips till episode 310 where the moving average starts to increase again until episode 368 where it peaks and dips all the way till episode 471 where it starts to increase drastically all the way to episode 500.\n",
    "\n",
    "I chose to use the model saved at episode 475 as there was a steep increase in moving average around episode 475 indicating that the rewards around those episodes are really high as they started pulling up the moving average. Therefore, the model saved around here should be good. I also experimented with other models saved at different episodes but they could not get the same results as the model at episode 475 thus, I will be using that for evaluation.\n",
    "\n",
    "The model at episode 475 achieved a constant reward around 220 which is great as according to the documentation for the Lunar Lander environment, above a 200 reward is considered solved. Therefore, the baseline model is able to solve the lunar lander environment and even get +20 reward above its 'solved' state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "Green"
         },
         "mode": "markers+lines",
         "name": "Avg Reward",
         "text": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48",
          "49",
          "50",
          "51",
          "52",
          "53",
          "54",
          "55",
          "56",
          "57",
          "58",
          "59",
          "60",
          "61",
          "62",
          "63",
          "64",
          "65",
          "66",
          "67",
          "68",
          "69",
          "70",
          "71",
          "72",
          "73",
          "74",
          "75",
          "76",
          "77",
          "78",
          "79",
          "80",
          "81",
          "82",
          "83",
          "84",
          "85",
          "86",
          "87",
          "88",
          "89",
          "90",
          "91",
          "92",
          "93",
          "94",
          "95",
          "96",
          "97",
          "98",
          "99",
          "100",
          "101",
          "102",
          "103",
          "104",
          "105",
          "106",
          "107",
          "108",
          "109",
          "110",
          "111",
          "112",
          "113",
          "114",
          "115",
          "116",
          "117",
          "118",
          "119",
          "120",
          "121",
          "122",
          "123",
          "124",
          "125",
          "126",
          "127",
          "128",
          "129",
          "130",
          "131",
          "132",
          "133",
          "134",
          "135",
          "136",
          "137",
          "138",
          "139",
          "140",
          "141",
          "142",
          "143",
          "144",
          "145",
          "146",
          "147",
          "148",
          "149",
          "150",
          "151",
          "152",
          "153",
          "154",
          "155",
          "156",
          "157",
          "158",
          "159",
          "160",
          "161",
          "162",
          "163",
          "164",
          "165",
          "166",
          "167",
          "168",
          "169",
          "170",
          "171",
          "172",
          "173",
          "174",
          "175",
          "176",
          "177",
          "178",
          "179",
          "180",
          "181",
          "182",
          "183",
          "184",
          "185",
          "186",
          "187",
          "188",
          "189",
          "190",
          "191",
          "192",
          "193",
          "194",
          "195",
          "196",
          "197",
          "198",
          "199",
          "200",
          "201",
          "202",
          "203",
          "204",
          "205",
          "206",
          "207",
          "208",
          "209",
          "210",
          "211",
          "212",
          "213",
          "214",
          "215",
          "216",
          "217",
          "218",
          "219",
          "220",
          "221",
          "222",
          "223",
          "224",
          "225",
          "226",
          "227",
          "228",
          "229",
          "230",
          "231",
          "232",
          "233",
          "234",
          "235",
          "236",
          "237",
          "238",
          "239",
          "240",
          "241",
          "242",
          "243",
          "244",
          "245",
          "246",
          "247",
          "248",
          "249",
          "250",
          "251",
          "252",
          "253",
          "254",
          "255",
          "256",
          "257",
          "258",
          "259",
          "260",
          "261",
          "262",
          "263",
          "264",
          "265",
          "266",
          "267",
          "268",
          "269",
          "270",
          "271",
          "272",
          "273",
          "274",
          "275",
          "276",
          "277",
          "278",
          "279",
          "280",
          "281",
          "282",
          "283",
          "284",
          "285",
          "286",
          "287",
          "288",
          "289",
          "290",
          "291",
          "292",
          "293",
          "294",
          "295",
          "296",
          "297",
          "298",
          "299",
          "300",
          "301",
          "302",
          "303",
          "304",
          "305",
          "306",
          "307",
          "308",
          "309",
          "310",
          "311",
          "312",
          "313",
          "314",
          "315",
          "316",
          "317",
          "318",
          "319",
          "320",
          "321",
          "322",
          "323",
          "324",
          "325",
          "326",
          "327",
          "328",
          "329",
          "330",
          "331",
          "332",
          "333",
          "334",
          "335",
          "336",
          "337",
          "338",
          "339",
          "340",
          "341",
          "342",
          "343",
          "344",
          "345",
          "346",
          "347",
          "348",
          "349",
          "350",
          "351",
          "352",
          "353",
          "354",
          "355",
          "356",
          "357",
          "358",
          "359",
          "360",
          "361",
          "362",
          "363",
          "364",
          "365",
          "366",
          "367",
          "368",
          "369",
          "370",
          "371",
          "372",
          "373",
          "374",
          "375",
          "376",
          "377",
          "378",
          "379",
          "380",
          "381",
          "382",
          "383",
          "384",
          "385",
          "386",
          "387",
          "388",
          "389",
          "390",
          "391",
          "392",
          "393",
          "394",
          "395",
          "396",
          "397",
          "398",
          "399",
          "400",
          "401",
          "402",
          "403",
          "404",
          "405",
          "406",
          "407",
          "408",
          "409",
          "410",
          "411",
          "412",
          "413",
          "414",
          "415",
          "416",
          "417",
          "418",
          "419",
          "420",
          "421",
          "422",
          "423",
          "424",
          "425",
          "426",
          "427",
          "428",
          "429",
          "430",
          "431",
          "432",
          "433",
          "434",
          "435",
          "436",
          "437",
          "438",
          "439",
          "440",
          "441",
          "442",
          "443",
          "444",
          "445",
          "446",
          "447",
          "448",
          "449",
          "450",
          "451",
          "452",
          "453",
          "454",
          "455",
          "456",
          "457",
          "458",
          "459",
          "460",
          "461",
          "462",
          "463",
          "464",
          "465",
          "466",
          "467",
          "468",
          "469",
          "470",
          "471",
          "472",
          "473",
          "474",
          "475",
          "476",
          "477",
          "478",
          "479",
          "480",
          "481",
          "482",
          "483",
          "484",
          "485",
          "486",
          "487",
          "488",
          "489",
          "490",
          "491",
          "492",
          "493",
          "494",
          "495",
          "496",
          "497",
          "498",
          "499",
          "500"
         ],
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500
         ],
         "xaxis": "x",
         "y": [
          -263.75170537211903,
          -362.91910365392687,
          -277.68345026524804,
          -308.23008961419845,
          -325.18578233649487,
          -307.73366132297974,
          -301.3337108981264,
          -269.4346449970228,
          -258.7651103555159,
          -245.7782225544792,
          -214.53732687104664,
          -215.79726153767123,
          -206.72890154418783,
          -218.91954811594547,
          -215.2798854668356,
          -213.43018246031104,
          -206.43687549379948,
          -205.84086271133933,
          -199.08566930904476,
          -194.0016139934432,
          -197.66381589898097,
          -194.2879267528109,
          -192.0417275355612,
          -187.51777304420844,
          -182.39003668178765,
          -179.15447981438498,
          -179.21061631651867,
          -176.28222506508305,
          -170.9910705074819,
          -169.1528793597931,
          -166.88636913368975,
          -163.61422986675598,
          -162.98368005327632,
          -160.37056637196278,
          -156.51461632628107,
          -154.05030905036315,
          -151.58796857863925,
          -149.16044451170958,
          -146.52362837223407,
          -159.28735001128092,
          -156.68008294447628,
          -158.30277799640095,
          -155.84555210431157,
          -156.1144289757263,
          -157.63618733255112,
          -160.60240658903277,
          -164.58635682694228,
          -166.09823356382086,
          -167.949987851421,
          -166.35083162557635,
          -167.2573492916654,
          -167.2659023429523,
          -166.61727220992208,
          -164.2132519967515,
          -161.83054642085278,
          -159.5337439339863,
          -157.98835596895512,
          -157.81162556644983,
          -158.54246021302356,
          -160.053761335042,
          -157.87171020949165,
          -156.33533871888852,
          -154.59262613000126,
          -152.7807416115251,
          -151.3352772698423,
          -150.39196428376982,
          -148.904065741756,
          -147.69237719153963,
          -147.68970199879766,
          -146.4918988998617,
          -145.40566381457884,
          -144.8922948317454,
          -144.3910095631942,
          -143.43699911314016,
          -142.27096678516068,
          -141.03240408064482,
          -139.4871030304399,
          -138.3275581400288,
          -137.54839884465682,
          -137.42558471787373,
          -136.95625150065746,
          -135.95416896529957,
          -134.6920918065073,
          -133.76743011862337,
          -132.9900673131395,
          -132.36485609728743,
          -131.1384865818761,
          -129.70942539229574,
          -128.60287263048633,
          -127.74218083431371,
          -127.00181150206284,
          -126.50418313167852,
          -125.77549289903008,
          -125.0058860706815,
          -126.26211546991163,
          -124.98272428757076,
          -124.0652193231678,
          -122.88004837550451,
          -122.42088470470046,
          -124.78026540438286,
          -123.62537135294724,
          -119.07496430468368,
          -112.78109626064251,
          -111.43021979323385,
          -107.4404404768133,
          -104.21059609355518,
          -102.35258095921375,
          -100.04991844630068,
          -100.096495638034,
          -98.99190502462832,
          -96.19387817113768,
          -97.32396222355652,
          -95.16228036266914,
          -94.12224590124784,
          -90.7528129315305,
          -89.69878007880153,
          -87.65806154472833,
          -84.55106298562026,
          -80.24515810167476,
          -78.28142149928154,
          -76.34043313679234,
          -74.0631034988456,
          -72.30383981888387,
          -71.19264723200381,
          -74.27131615292575,
          -76.89384376352339,
          -77.30162330310905,
          -74.95527463284063,
          -73.53216127728457,
          -70.94910336597276,
          -67.91251773396837,
          -67.42993652952592,
          -64.67707066560557,
          -63.50398076479517,
          -60.53696389050967,
          -58.58100650576543,
          -57.690178802495325,
          -55.97323963527224,
          -53.82750564510283,
          -51.91071468425843,
          -43.7481004089502,
          -40.82548621100426,
          -36.40209160935312,
          -35.452201613318344,
          -33.5422449601395,
          -29.07662937884637,
          -23.99920362480774,
          -20.528185207944794,
          -16.141061621080436,
          -11.550302504372594,
          -8.759010480626612,
          -6.118682461412528,
          -2.291088016411685,
          0.698720740109222,
          0.8733344858247722,
          3.663911120284928,
          4.173835692252462,
          4.858924453423006,
          5.706091158350874,
          9.439582235347912,
          11.440855091434065,
          13.07431493959716,
          15.935182756184078,
          17.67692827153453,
          20.08866619808834,
          22.92217556892183,
          26.259826631653493,
          29.39154658726724,
          32.54011642781094,
          27.835312351826225,
          30.079721060464284,
          32.825386179969136,
          36.58988508352825,
          39.72369345060057,
          42.60947194789611,
          45.87243642892082,
          48.6203680476198,
          51.21296496678725,
          54.2712882927261,
          57.25092698656126,
          59.28537939619727,
          60.63079223625476,
          63.21338315166126,
          66.02142728463859,
          67.78163022814674,
          70.5141994919373,
          73.30863305030636,
          76.1292277077222,
          77.1494700992368,
          78.385837239416,
          80.18098904393531,
          82.95472202316489,
          86.15451056502327,
          89.69903734536652,
          91.68870138551112,
          96.24347242881518,
          98.32043121566164,
          100.7226091117438,
          103.05592445523811,
          106.15613831347537,
          112.09486509918176,
          114.78406957933984,
          113.41196566810436,
          114.44888863703406,
          116.38503847794053,
          116.938481961118,
          120.11659265984916,
          119.9080900856236,
          122.50850895908076,
          125.74147149973535,
          128.0597066671526,
          128.83328078670723,
          128.71230825525146,
          126.7025855718854,
          124.34747618452012,
          127.12895316857868,
          129.77349054056674,
          129.33951033655714,
          123.91084210465476,
          123.98566613299518,
          122.7547718201352,
          124.13946648221778,
          125.1170723092482,
          126.23718687261156,
          128.9614612185363,
          132.90791852563433,
          135.87133087387414,
          139.3957808784357,
          140.66477325024232,
          142.87621557238637,
          142.26418556164722,
          139.6217275894297,
          140.34314523761455,
          140.30315866395244,
          142.93310332201247,
          142.9590044904568,
          143.64942287612422,
          145.26587328955878,
          143.71683430495003,
          142.1199580257834,
          143.17972318072432,
          143.9218396137678,
          141.78457191771085,
          140.61594069377645,
          141.94913501440104,
          142.23905862671606,
          140.05047930870757,
          139.91411410718587,
          142.6553356859404,
          143.0813636398026,
          143.2151004277365,
          143.0498362004592,
          144.13231021421154,
          142.89219233943084,
          143.58729247413547,
          146.32445126568535,
          146.26266724444,
          148.43022438191423,
          150.55896341262718,
          152.56679617523668,
          153.25674083609243,
          155.67852293820658,
          156.54061733000017,
          156.83976242589793,
          155.080380056597,
          153.09270459969403,
          150.41809098895922,
          149.56261017139062,
          149.2611924979464,
          148.9056977844994,
          157.5687787894352,
          154.0799903911778,
          152.9200261231003,
          149.83291521006225,
          149.52746628574246,
          147.68883080581062,
          145.16129418821563,
          143.3428573314582,
          140.8424202530783,
          141.17035400406922,
          140.5725701422505,
          141.94719862543786,
          143.980877530784,
          141.3810603569625,
          138.53598149786038,
          136.69947618226055,
          137.25045742028053,
          136.2486739650973,
          135.77286027362686,
          134.19094523891363,
          135.35505602572997,
          136.59974757128768,
          136.4318184216965,
          136.90966743759486,
          136.2714421456821,
          136.75729358422973,
          136.75229142333782,
          137.2551659957445,
          134.17767187916982,
          132.52192507148953,
          129.38151360344116,
          126.89316678136528,
          123.6002994934972,
          125.3585159063437,
          122.70201576108514,
          120.39212430638428,
          122.26983576563948,
          122.19919474102684,
          122.20642457764048,
          119.71259394028776,
          119.82235515355406,
          120.78133274549938,
          121.4185494791088,
          124.14840116559344,
          126.75718716621306,
          129.73190201790598,
          127.06923400697416,
          125.48475063046833,
          127.68399610933004,
          133.6078492014479,
          133.79655994993465,
          136.44674212643156,
          136.85891269890814,
          138.73070281675902,
          137.04319751138084,
          134.85810089336766,
          137.67479020447854,
          140.66608410429157,
          140.76921471496905,
          141.8353698264153,
          139.44923396450596,
          140.44116346414248,
          143.5102045230033,
          145.56425793628114,
          145.86518528658095,
          145.9179006638034,
          146.30764048656368,
          146.65166306784803,
          147.37609803069856,
          150.26754415102084,
          152.7565849239233,
          150.581303243759,
          150.88198226534163,
          153.2098435897243,
          154.91644520793128,
          155.18864753265362,
          154.8050579719996,
          157.23010595138172,
          157.67107562339478,
          157.34252428375677,
          157.47288074789924,
          157.89405230378566,
          158.94318185539237,
          159.9507510639845,
          159.9827670268477,
          160.18388209861146,
          160.1862419432399,
          160.07930529535267,
          160.15900508200636,
          157.7883823508443,
          159.26861099422447,
          158.3936932825383,
          158.92631891026562,
          158.85232778149629,
          158.6669149662157,
          158.85157340694627,
          158.93871020974836,
          161.76899351869122,
          160.65697712121406,
          160.35827184808804,
          155.72042230322452,
          151.68683113480506,
          150.36887405267885,
          146.81812320730228,
          144.25659909245363,
          139.35532934601872,
          135.56733123494354,
          132.8699294238986,
          130.61671099031787,
          129.2807025075553,
          126.2084072478657,
          122.4118583152073,
          122.01207804696564,
          119.59737875697492,
          120.23819135588924,
          122.68681890280097,
          122.54693862182494,
          119.98368698676548,
          119.22697409760676,
          116.27538525130493,
          119.41954218001526,
          116.97577557939324,
          116.85651220301077,
          114.11715942033372,
          110.5231553020278,
          106.87206325509052,
          102.43262399443128,
          97.79075118734208,
          93.05973014898656,
          92.64459009390242,
          91.21364308872796,
          91.08734309895166,
          93.54018351431378,
          96.80632392345456,
          96.59097199243938,
          98.56292315587427,
          98.80617069264828,
          98.70568862113404,
          96.48794556871152,
          96.3556081721306,
          98.93408427175456,
          98.60993234657424,
          93.6391595364398,
          92.40123739922568,
          92.76546387026892,
          94.37542214286098,
          96.03918013737132,
          98.65931828941866,
          100.27235620841762,
          99.4897348620516,
          97.28077081001184,
          97.07723969152345,
          97.04906753487408,
          96.92234974531765,
          94.46109370146132,
          96.56511573736142,
          98.8980856125312,
          98.5398146387599,
          97.16726913068294,
          96.99440240653335,
          94.09797743402731,
          96.42463738030622,
          93.73036237252532,
          93.5436949997584,
          93.99872736775968,
          91.41093557931823,
          91.21740756999964,
          90.43149440170671,
          89.34075415922854,
          87.27359946651886,
          85.09809154106613,
          82.64513926626344,
          84.6742719655838,
          84.53959941423946,
          81.572215522515,
          78.87093205249293,
          79.55968295690568,
          81.73836712246388,
          81.3738913977842,
          79.99906126788431,
          80.3363488441888,
          80.30963245541368,
          78.2348616827634,
          77.8838704263705,
          77.94534273884669,
          79.62793247711022,
          77.52790737520722,
          76.0747933665902,
          74.2888884154561,
          72.31780069739965,
          75.37307198545999,
          75.33758091038013,
          75.32854641035479,
          75.51956435496223,
          75.50230439624094,
          75.70700997182439,
          78.75717753993641,
          77.94386405993237,
          75.9344729137028,
          74.08512426691513,
          73.0636626337562,
          77.6738485202335,
          81.79812024992641,
          85.79361684314458,
          90.83950291392892,
          96.37476866475588,
          102.0299883309283,
          108.125469872876,
          112.9585074103294,
          115.32937043541412,
          119.16351433431288,
          119.14406837179004,
          121.95778096152732,
          122.89203391400554,
          125.8732775473659,
          125.92837998810668,
          126.4191444738205,
          127.41378996803913,
          130.17014988616637,
          130.61960519096158,
          131.42340014752634,
          131.40088767267684,
          134.38127616737248,
          134.59028958206045,
          138.0052172827349,
          141.27575494770764,
          145.35891761488168,
          150.58278367359836,
          155.64643480869282,
          158.3939991282521
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "Red"
         },
         "mode": "markers+lines",
         "name": "Reward",
         "text": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48",
          "49",
          "50",
          "51",
          "52",
          "53",
          "54",
          "55",
          "56",
          "57",
          "58",
          "59",
          "60",
          "61",
          "62",
          "63",
          "64",
          "65",
          "66",
          "67",
          "68",
          "69",
          "70",
          "71",
          "72",
          "73",
          "74",
          "75",
          "76",
          "77",
          "78",
          "79",
          "80",
          "81",
          "82",
          "83",
          "84",
          "85",
          "86",
          "87",
          "88",
          "89",
          "90",
          "91",
          "92",
          "93",
          "94",
          "95",
          "96",
          "97",
          "98",
          "99",
          "100",
          "101",
          "102",
          "103",
          "104",
          "105",
          "106",
          "107",
          "108",
          "109",
          "110",
          "111",
          "112",
          "113",
          "114",
          "115",
          "116",
          "117",
          "118",
          "119",
          "120",
          "121",
          "122",
          "123",
          "124",
          "125",
          "126",
          "127",
          "128",
          "129",
          "130",
          "131",
          "132",
          "133",
          "134",
          "135",
          "136",
          "137",
          "138",
          "139",
          "140",
          "141",
          "142",
          "143",
          "144",
          "145",
          "146",
          "147",
          "148",
          "149",
          "150",
          "151",
          "152",
          "153",
          "154",
          "155",
          "156",
          "157",
          "158",
          "159",
          "160",
          "161",
          "162",
          "163",
          "164",
          "165",
          "166",
          "167",
          "168",
          "169",
          "170",
          "171",
          "172",
          "173",
          "174",
          "175",
          "176",
          "177",
          "178",
          "179",
          "180",
          "181",
          "182",
          "183",
          "184",
          "185",
          "186",
          "187",
          "188",
          "189",
          "190",
          "191",
          "192",
          "193",
          "194",
          "195",
          "196",
          "197",
          "198",
          "199",
          "200",
          "201",
          "202",
          "203",
          "204",
          "205",
          "206",
          "207",
          "208",
          "209",
          "210",
          "211",
          "212",
          "213",
          "214",
          "215",
          "216",
          "217",
          "218",
          "219",
          "220",
          "221",
          "222",
          "223",
          "224",
          "225",
          "226",
          "227",
          "228",
          "229",
          "230",
          "231",
          "232",
          "233",
          "234",
          "235",
          "236",
          "237",
          "238",
          "239",
          "240",
          "241",
          "242",
          "243",
          "244",
          "245",
          "246",
          "247",
          "248",
          "249",
          "250",
          "251",
          "252",
          "253",
          "254",
          "255",
          "256",
          "257",
          "258",
          "259",
          "260",
          "261",
          "262",
          "263",
          "264",
          "265",
          "266",
          "267",
          "268",
          "269",
          "270",
          "271",
          "272",
          "273",
          "274",
          "275",
          "276",
          "277",
          "278",
          "279",
          "280",
          "281",
          "282",
          "283",
          "284",
          "285",
          "286",
          "287",
          "288",
          "289",
          "290",
          "291",
          "292",
          "293",
          "294",
          "295",
          "296",
          "297",
          "298",
          "299",
          "300",
          "301",
          "302",
          "303",
          "304",
          "305",
          "306",
          "307",
          "308",
          "309",
          "310",
          "311",
          "312",
          "313",
          "314",
          "315",
          "316",
          "317",
          "318",
          "319",
          "320",
          "321",
          "322",
          "323",
          "324",
          "325",
          "326",
          "327",
          "328",
          "329",
          "330",
          "331",
          "332",
          "333",
          "334",
          "335",
          "336",
          "337",
          "338",
          "339",
          "340",
          "341",
          "342",
          "343",
          "344",
          "345",
          "346",
          "347",
          "348",
          "349",
          "350",
          "351",
          "352",
          "353",
          "354",
          "355",
          "356",
          "357",
          "358",
          "359",
          "360",
          "361",
          "362",
          "363",
          "364",
          "365",
          "366",
          "367",
          "368",
          "369",
          "370",
          "371",
          "372",
          "373",
          "374",
          "375",
          "376",
          "377",
          "378",
          "379",
          "380",
          "381",
          "382",
          "383",
          "384",
          "385",
          "386",
          "387",
          "388",
          "389",
          "390",
          "391",
          "392",
          "393",
          "394",
          "395",
          "396",
          "397",
          "398",
          "399",
          "400",
          "401",
          "402",
          "403",
          "404",
          "405",
          "406",
          "407",
          "408",
          "409",
          "410",
          "411",
          "412",
          "413",
          "414",
          "415",
          "416",
          "417",
          "418",
          "419",
          "420",
          "421",
          "422",
          "423",
          "424",
          "425",
          "426",
          "427",
          "428",
          "429",
          "430",
          "431",
          "432",
          "433",
          "434",
          "435",
          "436",
          "437",
          "438",
          "439",
          "440",
          "441",
          "442",
          "443",
          "444",
          "445",
          "446",
          "447",
          "448",
          "449",
          "450",
          "451",
          "452",
          "453",
          "454",
          "455",
          "456",
          "457",
          "458",
          "459",
          "460",
          "461",
          "462",
          "463",
          "464",
          "465",
          "466",
          "467",
          "468",
          "469",
          "470",
          "471",
          "472",
          "473",
          "474",
          "475",
          "476",
          "477",
          "478",
          "479",
          "480",
          "481",
          "482",
          "483",
          "484",
          "485",
          "486",
          "487",
          "488",
          "489",
          "490",
          "491",
          "492",
          "493",
          "494",
          "495",
          "496",
          "497",
          "498",
          "499",
          "500"
         ],
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500
         ],
         "xaxis": "x",
         "y": [
          -263.75170537211903,
          -462.08650193573465,
          -107.21214348789034,
          -399.8700076610497,
          -393.0085532256806,
          -220.47305625540412,
          -262.9340083490064,
          -46.14118368929725,
          -173.40883322346082,
          -128.89623234514912,
          97.87162996327912,
          -229.65654287054167,
          -97.90858162238732,
          -377.3979535487948,
          -164.3246083792968,
          -185.68463736244303,
          -94.54396402961449,
          -195.70864540951644,
          -77.49218806774219,
          -97.4045629970138,
          -270.90785400973687,
          -123.3942546832393,
          -142.62534475606856,
          -83.46681974309517,
          -59.32436398368811,
          -98.26555812931863,
          -180.6701653719945,
          -97.21566127632138,
          -22.83874289464954,
          -115.84533607681828,
          -98.891062350589,
          -62.177912591808926,
          -142.80608602192785,
          -74.13781488861625,
          -25.41231477310208,
          -67.79955439323643,
          -62.943711596578495,
          -59.34205403531152,
          -46.32461507216489,
          -657.0724939341077,
          -52.38940027229029,
          -224.83327512531167,
          -52.642064636558,
          -167.67613444656033,
          -224.59355503284303,
          -294.08227313070694,
          -347.8480677707791,
          -237.1564401971152,
          -256.834193656229,
          -87.99217655918734,
          -212.5832325961185,
          -167.70210795858236,
          -132.8885052923507,
          -36.8001806987105,
          -33.16444532232331,
          -33.20960715633003,
          -71.44662992720863,
          -147.73799262364767,
          -200.93086971430097,
          -249.2205275341292,
          -26.9486426764711,
          -62.61667779209951,
          -46.54444561899077,
          -38.632016947524406,
          -58.825559402140414,
          -89.07662018906277,
          -50.70276196884362,
          -66.509244327043,
          -147.50778889234385,
          -63.84348507328171,
          -69.36920784477829,
          -108.44309705056985,
          -108.29847022750614,
          -73.79423625919742,
          -55.98457451468036,
          -48.14020124195511,
          -22.04422321486572,
          -49.04260157837511,
          -76.7739738056408,
          -127.72326870200912,
          -99.40959412335344,
          -54.7854836013116,
          -31.201764785541343,
          -57.02051002425672,
          -67.69159165249447,
          -79.22190274986279,
          -25.67070825650005,
          -5.381101898807394,
          -31.226229591259017,
          -51.14061097494872,
          -60.368571599484895,
          -81.2200014267031,
          -58.73599149537382,
          -53.43245103426301,
          -244.3476789975446,
          -3.440561965189022,
          -35.9847427404847,
          -7.918466452166823,
          -77.42284496589991,
          -358.3589546729426,
          -8.135966209385973,
          195.83940650250264,
          173.5941705124226,
          29.226379720382717,
          3.097703297427347,
          -66.79427051660957,
          -32.81352768692143,
          -30.365094544784757,
          -50.845480054361936,
          -61.84518126948713,
          153.70447985740597,
          -16.26685933102583,
          -11.326674920914874,
          7.134898981163791,
          -37.08522360734303,
          -57.86729025367256,
          20.427934578951024,
          219.2628904403005,
          239.18774786897836,
          120.8452087739752,
          98.63526161439306,
          -40.8975605771158,
          54.2913769928945,
          -30.39489348118028,
          -394.412380756212,
          -324.19965265404903,
          -139.45129162747068,
          56.31105032511623,
          46.51878763483944,
          238.050106147844,
          190.84981275562467,
          -50.15036070190136,
          215.86153966414645,
          -24.32400604007767,
          225.53088941422,
          172.13938108606672,
          22.174043637044072,
          110.46714429295278,
          157.37707897179948,
          147.2712719731196,
          167.35154787202364,
          242.79463372024728,
          221.92957964145413,
          43.296824962954055,
          25.229487524503703,
          226.4336186777631,
          218.73772802719463,
          2.724792332378416,
          205.9430420761848,
          206.8324771312633,
          193.9283178391562,
          54.0898973445046,
          218.88493098650244,
          169.08217911626113,
          -19.16419238144005,
          248.68379475815263,
          18.292774612390865,
          -2.252665048983494,
          -62.174155425933286,
          176.1517290623999,
          -47.09196906942768,
          138.03080198800149,
          226.33097168317903,
          129.37185143140437,
          204.9535136344112,
          227.3588870520415,
          248.02613714683483,
          265.6009535481445,
          251.4963095678722,
          -622.693000566801,
          162.8417944991618,
          207.94296922521272,
          271.7712922089004,
          208.216174846798,
          217.669391967652,
          273.57483806881527,
          229.40089224664223,
          239.80806562104672,
          259.8480543414488,
          224.16953427171129,
          77.7564246712279,
          36.47710272245285,
          206.0561988547456,
          252.4106926451685,
          120.7599872700676,
          208.29790399035176,
          203.015886645411,
          259.20935214249823,
          97.66337964416851,
          93.64685156683896,
          130.16972128150226,
          219.7784593027021,
          241.95864130099525,
          299.2612133192932,
          147.52361702034244,
          215.68419637616515,
          206.33227550630312,
          206.63522476381388,
          227.74638324076088,
          235.6987547160614,
          241.4524506834032,
          263.47368628657955,
          57.256911467718616,
          278.3233903743233,
          224.77751365193484,
          58.9954950983516,
          254.19491005523727,
          -53.87228768370075,
          232.27721167438847,
          275.68373655175014,
          172.2965706396554,
          231.8354659324255,
          -28.48508500806065,
          -214.30866594088357,
          -230.7311491427315,
          243.84395178257097,
          209.2309843171208,
          -23.40406602601924,
          -329.0326009818397,
          246.74497473136176,
          -3.4751168248815247,
          238.48942248473355,
          57.84062795295455,
          167.42294789259512,
          244.7568154572185,
          4.179807260687539,
          -24.895005481829543,
          216.51815883324903,
          184.47927987758737,
          269.8744621713895,
          176.2350750631892,
          -76.03844243834797,
          22.71282176476985,
          211.8228957242761,
          241.30040442398436,
          228.14690742709536,
          241.87163803847827,
          185.4355353939344,
          -45.98579315252774,
          -3.907425224035221,
          254.30755262215476,
          242.30530760941264,
          26.930596418493984,
          103.8978260240812,
          177.94945134603705,
          54.51177236831851,
          5.3871075589072,
          204.964842673504,
          279.5881717865842,
          248.97186541626783,
          220.33989271258545,
          177.23663088415014,
          163.41977273349164,
          93.63302563365026,
          239.287292721431,
          257.2888455651013,
          242.4436086123681,
          237.21604549728863,
          212.74997705302405,
          140.61695359762732,
          245.83613980883,
          197.5080232440999,
          225.1023355591578,
          256.5446263688524,
          -48.32576786798772,
          4.198292487210722,
          -42.77708763217609,
          161.62257457240463,
          235.1577685302792,
          215.59134350972465,
          252.278180931714,
          -189.525833724838,
          90.78657814938998,
          -40.026910007946896,
          177.36583349050176,
          31.967208494537942,
          18.29363969172124,
          45.73876971413938,
          -12.736079295320152,
          292.9693631915304,
          163.79336422801805,
          216.59390147315216,
          241.87867216241543,
          -56.52533570122664,
          -34.9422721241487,
          -64.72704960551377,
          263.9470090303706,
          101.83575767190705,
          211.1521693039792,
          -62.11003886186522,
          211.2220410352881,
          255.88356738283227,
          202.81761519399143,
          290.22139190673033,
          234.8004588361059,
          196.5946123136505,
          215.1789781260844,
          257.12260731937545,
          -104.1916810102267,
          60.515955665048864,
          -81.48280355682216,
          -9.870578346260489,
          -69.10590978809749,
          234.83676916521463,
          10.016875703209408,
          -8.521523272852946,
          248.6443524831273,
          247.0601665693628,
          -53.14207418572134,
          -19.599682698235853,
          286.7696190916415,
          269.1533074261338,
          296.1943560269771,
          247.2299353268868,
          49.17872012169943,
          69.71505087825474,
          -25.08551732154268,
          49.19816329003041,
          198.71972733901367,
          269.27656132206107,
          265.8047603285252,
          264.19328300130786,
          280.11865030486666,
          246.89142985589217,
          -3.015087950600247,
          24.06205703788889,
          288.66542768288537,
          277.2256783992888,
          226.9343505116746,
          292.1609461336594,
          28.874740118545304,
          276.4199545264761,
          233.9347045065974,
          230.17221650583096,
          242.21655810455505,
          246.62465752345287,
          267.5106295258844,
          276.6179187481969,
          258.60346664183714,
          246.05026500002245,
          247.4856928391129,
          34.6041029255629,
          272.6738887892591,
          262.0445901811422,
          276.2645894629859,
          205.4418861429911,
          15.769226742262106,
          250.3169534765002,
          249.5027795468258,
          246.40448648314293,
          262.13786829466017,
          262.87821985711287,
          283.19871559642775,
          265.1842628013005,
          96.86663788283326,
          259.5999149695665,
          257.5271898725764,
          231.6430071757564,
          245.2657239493072,
          -26.68291879434244,
          290.1200465790247,
          157.46945092852772,
          251.30321164455967,
          217.62923155345965,
          237.81793202551057,
          -29.675265354200505,
          12.999109570223029,
          243.08152657105052,
          49.30891842721184,
          204.98853594455025,
          -252.8314605214892,
          -155.1145270786535,
          -322.6394990195841,
          -267.8392572336444,
          -298.74084560765823,
          -317.66241089942577,
          -350.62060072405166,
          -254.1439432238193,
          -181.8362920775148,
          -147.67293605434165,
          -17.332458037119395,
          -219.6580779704836,
          176.21609438074591,
          -2.005956126647689,
          8.196736789121445,
          212.3691101139352,
          -78.85495798409082,
          5.058593889364559,
          25.407755866878432,
          -86.95830417250846,
          255.4498109378816,
          -35.59838562753802,
          243.83796636820463,
          -73.85701585639193,
          -72.7730240421682,
          -133.95983790455898,
          -251.7887530129321,
          -253.65017538992663,
          -220.7105175545321,
          -146.1208265737248,
          -84.00969185757195,
          -94.2391025242264,
          237.86630360531476,
          260.7742715351192,
          213.08622413268276,
          209.18394321013105,
          16.04647794132174,
          238.49566326018777,
          23.06811827468981,
          -66.50815124039457,
          240.82640336378395,
          254.03027464843075,
          -232.8947463974458,
          171.164220168351,
          284.0168089022527,
          211.78450565350045,
          237.75460832379997,
          239.5484360352392,
          212.1149931089252,
          119.67497135604418,
          46.17119206604556,
          245.2481173611963,
          261.3478951797215,
          267.3201535596698,
          -1.695430573599264,
          209.49113767531213,
          259.69201443003686,
          252.4800593319825,
          138.59858208351557,
          209.474811372569,
          -0.377976089453218,
          263.86739469271606,
          4.298178740605853,
          215.0812998571381,
          276.13048567395947,
          -19.150412528027708,
          227.07832858227297,
          188.13339952830088,
          166.45315425789812,
          49.8208426781579,
          26.32396452929549,
          -0.2624869159581461,
          239.54650555692075,
          259.0719611034811,
          -37.66118288302954,
          3.4349589907574405,
          275.0057274886784,
          235.81632746364025,
          213.5049052838523,
          110.64493642693851,
          280.470531689896,
          259.4395130283725,
          53.32637181943272,
          247.74859870074627,
          271.3929663613952,
          266.8082014474501,
          47.49737967736192,
          110.76267500225887,
          51.266607111211925,
          46.18586442560678,
          281.89948129975045,
          286.53544799595943,
          156.55696642596715,
          270.59602404991153,
          215.8859757226108,
          258.4931951594388,
          278.39165902511286,
          -69.14555191018458,
          40.13302080186568,
          -137.47529489834318,
          101.82091099549837,
          212.79731401271823,
          261.43691762033086,
          80.90565689544715,
          241.7952359155772,
          260.32099522586384,
          253.5147753839896,
          265.023035012665,
          233.9928480589732,
          57.62087345604303,
          239.57559773443307,
          -19.296500251924808,
          64.52689359298382,
          270.57564258104435,
          299.0996508427491,
          13.762083303939548,
          261.93632317102947,
          21.604236931989547,
          283.4509456202182,
          70.80274165119428,
          -5.7750135594681495,
          253.17605097808283,
          265.4208523367207,
          264.9483212516876,
          271.0506819117276,
          257.5512801200799,
          278.4395914800169,
          275.8217189174531,
          257.77858925461413,
          56.7934787209542
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Reward and Last 100 Average Reward",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 2000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('DQN_History.log', sep=';')\n",
    "plot_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reached the maximum number of steps. 450\n",
      "episode 1 finished in 450 steps with reward 147.77049373202067.\n",
      "episode 2 finished in 235 steps with reward 257.28173297949843.\n",
      "episode 3 finished in 330 steps with reward 287.9188121502981.\n",
      "Episode reached the maximum number of steps. 450\n",
      "episode 4 finished in 450 steps with reward 146.48165328935798.\n",
      "episode 5 finished in 334 steps with reward 256.3675295714852.\n",
      "episode 6 finished in 321 steps with reward 275.9922407034659.\n",
      "episode 7 finished in 212 steps with reward 275.05596113321457.\n",
      "Episode reached the maximum number of steps. 450\n",
      "episode 8 finished in 450 steps with reward 156.4268290289234.\n",
      "episode 9 finished in 170 steps with reward 264.44861794219787.\n",
      "Episode reached the maximum number of steps. 450\n",
      "episode 10 finished in 450 steps with reward 114.03661042287595.\n",
      "Average reward: 218.17804809533382\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "filename = \"Models/DQN/episode475.h5\"\n",
    "# 475, \n",
    "trained_model = load_model(filename, custom_objects={\"masked_huber_loss\": MeanSquaredError()})\n",
    "\n",
    "evaluation_max_episodes = 10\n",
    "evaluation_max_steps = 450\n",
    "\n",
    "def episode_trigger(x):\n",
    "    return x % 1 == 0\n",
    "env = wrappers.RecordVideo(env, f'Videos/DQN/', episode_trigger)\n",
    "\n",
    "def get_q_values(model, state):\n",
    "    input = state[np.newaxis, ...]\n",
    "    return model.predict(input)[0]\n",
    "\n",
    "def select_best_action(q_values):\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "rewards = []\n",
    "for episode in range(1, evaluation_max_episodes + 1):\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    step = 1\n",
    "    for step in range(1, evaluation_max_steps + 1):\n",
    "        env.render()\n",
    "        q_values = get_q_values(trained_model, state)\n",
    "        action = select_best_action(q_values)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if step == evaluation_max_steps:\n",
    "            print(f\"Episode reached the maximum number of steps. {evaluation_max_steps}\")\n",
    "            done = True\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"episode {episode} finished in {step} steps with reward {episode_reward}.\")\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 3rd episode as its the episode with the highest reward. Therefore I choose to display it\n",
    "\n",
    "<video controls=\"true\" allowfullscreen=\"true\">\n",
    "  <source src=\"Videos\\DQN\\rl-video-episode-3.mp4\">\n",
    "</video>\n",
    "\n",
    "You can see from the video that it agent is able to recognise that the lander was tilting left and veering to the right at the start. The agent then fired burst of the left thruster to counter the tilt of the lander so as to center it. The agent then recognises that its no longer in the center and decides to fire its right thrusters to direct the lander back into the middle, where its inbetween the flags and it manages to land smoothly inbetween the flags. This shows that the baseline RL model is able to successfully land the lunar lander even if it needs to make drastic corrective adjustments at the start to stablise and then center the lander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q Learning (DDQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our second improved model we implemented the double deep q learning network. The problem with the normal DQN is that it is prone to overestimation of q values for certain actions. This is due to the original target formula using the max function.\n",
    "\n",
    "This overestimation of q values will lead to the model to think that a certain action is the best value when it is not.\n",
    "\n",
    "To solve this problem we use two models. One for selecting actions and one for calculating the target values. \n",
    "\n",
    "The modified formula is shown below where thetha is the action selcection model and thetha prime is the target calculating model:\n",
    "\n",
    "<img src = 'https://cdn.discordapp.com/attachments/345195124374110218/944293972367515718/unknown.png'>\n",
    "\n",
    "The weights of the action selcection model are copied to the target calculating model every x steps.\n",
    "\n",
    "Our implementation of the DDQN is very similar to DQN but with just a slight modification to the learn function in the agent class. I will not be explaining the rest of the parts as I have explained it above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_count = 0\n",
    "        self.discrete = discrete\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        dtype = np.int8 if self.discrete else np.float32\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_count % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        # store one hot encoding of actions, if appropriate\n",
    "        if self.discrete:\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.mem_count += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_count, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states2 = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states2, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_huber_loss(mask_value, clip_delta):\n",
    "    def f(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) < clip_delta\n",
    "        mask_true = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "        masked_squared_error = 0.5 * K.square(mask_true * (y_true - y_pred))\n",
    "        linear_loss  = mask_true * (clip_delta * K.abs(error) - 0.5 * (clip_delta ** 2))\n",
    "        huber_loss = tf.where(cond, masked_squared_error, linear_loss)\n",
    "        return K.sum(huber_loss) / K.sum(mask_true)\n",
    "    f.__name__ = 'masked_huber_loss'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dqn(lr, n_actions, input_dims):\n",
    "    model = Sequential([\n",
    "                Dense(256, input_shape=(input_dims,)),\n",
    "                Activation('relu'),\n",
    "                Dense(256),\n",
    "                Activation('relu'),\n",
    "                Dense(n_actions)])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=masked_huber_loss(0,1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the initialization of the agent class, we have 2 networks of the same architecure. One is called q_eval which is our action selcection model and q_target whicn is target calculating model.\n",
    "\n",
    "We also have a new parameter called replace_target. This is the x step that can be set to replace the weights of the target calculating model with the current weights of the action selcection model.\n",
    "\n",
    "Inside of the learn function we use the modified target equation where we call the action selection model instead of the max operator to get the action in the q value of the target.\n",
    "\n",
    "We have a new function called update_network_parameters where we update the weights of the target calculation model every x steps. We call this function every x steps in the learn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,\n",
    "                 input_dims, epsilon_dec,  epsilon_min,\n",
    "                 mem_size, replace_target):\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.replace_target = replace_target\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions,\n",
    "                                   discrete=True)\n",
    "        self.q_eval = get_dqn(alpha, n_actions, input_dims)\n",
    "        self.q_target = get_dqn(alpha, n_actions, input_dims)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr > self.batch_size:\n",
    "            state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "            action_values = np.array(self.action_space, dtype=np.int8)\n",
    "            action_indices = np.dot(action, action_values)\n",
    "\n",
    "            q_next = self.q_target.predict(new_state)\n",
    "            q_eval = self.q_eval.predict(new_state)\n",
    "            q_pred = self.q_eval.predict(state)\n",
    "\n",
    "            max_actions = np.argmax(q_eval, axis=1)\n",
    "\n",
    "            q_target = q_pred\n",
    "\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "            q_target[batch_index, action_indices] = reward + self.gamma * q_next[batch_index, max_actions.astype(int)] * done\n",
    "\n",
    "            _ = self.q_eval.fit(state, q_target, verbose=0)\n",
    "\n",
    "            self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "            if self.memory.mem_cntr % self.replace_target == 0:\n",
    "                self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        self.q_target.set_weights(self.q_eval.get_weights())\n",
    "\n",
    "    def save_model(self, file):\n",
    "        self.q_eval.save(file)\n",
    "\n",
    "    def load_model(self, file):\n",
    "        self.q_eval = load_model(file)\n",
    "        # if we are in evaluation mode we want to use the best weights for\n",
    "        # q_target\n",
    "        if self.epsilon == 0.0:\n",
    "            self.update_network_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "ddqn_agent = DDQNAgent(alpha=0.0005, gamma=0.99, n_actions=4, epsilon=1.0, batch_size=64, input_dims=8, \n",
    "                        epsilon_dec=0.995, epsilon_min=0.01, mem_size=1000000, replace_target=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: -137.52  average score -137.52\n",
      "episode:  1 score: -395.91  average score -266.71\n",
      "episode:  2 score: -308.45  average score -280.63\n",
      "episode:  3 score: 11.93  average score -207.49\n",
      "episode:  4 score: -2.96  average score -166.58\n",
      "episode:  5 score: -99.42  average score -155.39\n",
      "episode:  6 score: -211.18  average score -163.36\n",
      "episode:  7 score: -199.06  average score -167.82\n",
      "episode:  8 score: -280.16  average score -180.30\n",
      "episode:  9 score: -143.75  average score -176.65\n",
      "episode:  10 score: -179.82  average score -176.93\n",
      "episode:  11 score: -174.84  average score -176.76\n",
      "episode:  12 score: -213.78  average score -179.61\n",
      "episode:  13 score: -206.45  average score -181.53\n",
      "episode:  14 score: -124.71  average score -177.74\n",
      "episode:  15 score: -67.08  average score -170.82\n",
      "episode:  16 score: -89.73  average score -166.05\n",
      "episode:  17 score: 6.12  average score -156.49\n",
      "episode:  18 score: -46.24  average score -150.68\n",
      "episode:  19 score: -94.53  average score -147.88\n",
      "episode:  20 score: -56.83  average score -143.54\n",
      "episode:  21 score: -96.98  average score -141.42\n",
      "episode:  22 score: -208.50  average score -144.34\n",
      "episode:  23 score: -71.11  average score -141.29\n",
      "episode:  24 score: -112.33  average score -140.13\n",
      "episode:  25 score: -93.98  average score -138.36\n",
      "episode:  26 score: -141.45  average score -138.47\n",
      "episode:  27 score: -78.34  average score -136.32\n",
      "episode:  28 score: -117.81  average score -135.68\n",
      "episode:  29 score: -108.87  average score -134.79\n",
      "episode:  30 score: -89.31  average score -133.32\n",
      "episode:  31 score: -103.61  average score -132.40\n",
      "episode:  32 score: -82.19  average score -130.87\n",
      "episode:  33 score: -244.76  average score -134.22\n",
      "episode:  34 score: -125.90  average score -133.99\n",
      "episode:  35 score: -94.52  average score -132.89\n",
      "episode:  36 score: -756.16  average score -149.73\n",
      "episode:  37 score: -265.82  average score -152.79\n",
      "episode:  38 score: -231.47  average score -154.81\n",
      "episode:  39 score: -236.04  average score -156.84\n",
      "episode:  40 score: -243.71  average score -158.96\n",
      "episode:  41 score: -88.15  average score -157.27\n",
      "episode:  42 score: -78.00  average score -155.43\n",
      "episode:  43 score: -194.06  average score -156.30\n",
      "episode:  44 score: -188.75  average score -157.03\n",
      "episode:  45 score: -52.80  average score -154.76\n",
      "episode:  46 score: -88.72  average score -153.36\n",
      "episode:  47 score: -111.18  average score -152.48\n",
      "episode:  48 score: -107.32  average score -151.55\n",
      "episode:  49 score: -169.93  average score -151.92\n",
      "episode:  50 score: -86.05  average score -150.63\n",
      "episode:  51 score: -57.27  average score -148.84\n",
      "episode:  52 score: -67.89  average score -147.31\n",
      "episode:  53 score: -61.19  average score -145.71\n",
      "episode:  54 score: -25.12  average score -143.52\n",
      "episode:  55 score: -89.02  average score -142.55\n",
      "episode:  56 score: -125.59  average score -142.25\n",
      "episode:  57 score: -64.86  average score -140.92\n",
      "episode:  58 score: -56.76  average score -139.49\n",
      "episode:  59 score: -33.17  average score -137.72\n",
      "episode:  60 score: -93.36  average score -136.99\n",
      "episode:  61 score: -68.99  average score -135.89\n",
      "episode:  62 score: -3.93  average score -133.80\n",
      "episode:  63 score: -10.38  average score -131.87\n",
      "episode:  64 score: -66.26  average score -130.86\n",
      "episode:  65 score: -50.25  average score -129.64\n",
      "episode:  66 score: -46.03  average score -128.39\n",
      "episode:  67 score: -102.16  average score -128.01\n",
      "episode:  68 score: -50.75  average score -126.89\n",
      "episode:  69 score: -67.82  average score -126.04\n",
      "episode:  70 score: -80.83  average score -125.41\n",
      "episode:  71 score: -35.39  average score -124.16\n",
      "episode:  72 score: -39.25  average score -122.99\n",
      "episode:  73 score: -46.31  average score -121.96\n",
      "episode:  74 score: -48.77  average score -120.98\n",
      "episode:  75 score: -249.96  average score -122.68\n",
      "episode:  76 score: -6.39  average score -121.17\n",
      "episode:  77 score: -13.17  average score -119.78\n",
      "episode:  78 score: -9.87  average score -118.39\n",
      "episode:  79 score: -13.73  average score -117.08\n",
      "episode:  80 score: -32.15  average score -116.03\n",
      "episode:  81 score: -2.59  average score -114.65\n",
      "episode:  82 score: 2.49  average score -113.24\n",
      "episode:  83 score: -55.79  average score -112.56\n",
      "episode:  84 score: -58.16  average score -111.92\n",
      "episode:  85 score: -49.05  average score -111.18\n",
      "episode:  86 score: -39.56  average score -110.36\n",
      "episode:  87 score: -67.67  average score -109.88\n",
      "episode:  88 score: -32.30  average score -109.00\n",
      "episode:  89 score: -32.12  average score -108.15\n",
      "episode:  90 score: -49.71  average score -107.51\n",
      "episode:  91 score: -19.03  average score -106.55\n",
      "episode:  92 score: -16.25  average score -105.58\n",
      "episode:  93 score: -61.74  average score -105.11\n",
      "episode:  94 score: -33.08  average score -104.35\n",
      "episode:  95 score: -190.49  average score -105.25\n",
      "episode:  96 score: -109.17  average score -105.29\n",
      "episode:  97 score: -90.73  average score -105.14\n",
      "episode:  98 score: -30.15  average score -104.38\n",
      "episode:  99 score: -65.60  average score -103.99\n",
      "episode:  100 score: -61.68  average score -103.58\n",
      "episode:  101 score: 6.85  average score -102.15\n",
      "episode:  102 score: 0.40  average score -98.22\n",
      "episode:  103 score: -131.11  average score -96.47\n",
      "episode:  104 score: 181.05  average score -94.79\n",
      "episode:  105 score: 162.96  average score -93.15\n",
      "episode:  106 score: 260.35  average score -89.59\n",
      "episode:  107 score: 25.31  average score -87.25\n",
      "episode:  108 score: 289.44  average score -82.41\n",
      "episode:  109 score: 215.80  average score -77.50\n",
      "episode:  110 score: -113.70  average score -77.20\n",
      "episode:  111 score: 225.03  average score -73.19\n",
      "episode:  112 score: -247.73  average score -73.91\n",
      "episode:  113 score: 131.47  average score -70.50\n",
      "episode:  114 score: -85.88  average score -69.30\n",
      "episode:  115 score: -141.69  average score -69.47\n",
      "episode:  116 score: -114.49  average score -69.94\n",
      "episode:  117 score: 138.98  average score -67.68\n",
      "episode:  118 score: -164.15  average score -69.36\n",
      "episode:  119 score: 94.12  average score -67.97\n",
      "episode:  120 score: -191.46  average score -68.93\n",
      "episode:  121 score: -173.82  average score -70.09\n",
      "episode:  122 score: -131.72  average score -70.43\n",
      "episode:  123 score: 72.71  average score -67.65\n",
      "episode:  124 score: -83.39  average score -67.77\n",
      "episode:  125 score: -87.09  average score -67.52\n",
      "episode:  126 score: -139.60  average score -67.97\n",
      "episode:  127 score: -25.79  average score -66.83\n",
      "episode:  128 score: 86.89  average score -65.19\n",
      "episode:  129 score: 275.97  average score -61.29\n",
      "episode:  130 score: 247.16  average score -57.77\n",
      "episode:  131 score: 264.76  average score -54.26\n",
      "episode:  132 score: 264.18  average score -50.62\n",
      "episode:  133 score: 257.04  average score -47.26\n",
      "episode:  134 score: -155.78  average score -46.38\n",
      "episode:  135 score: -44.05  average score -45.57\n",
      "episode:  136 score: 236.03  average score -42.30\n",
      "episode:  137 score: 209.09  average score -32.74\n",
      "episode:  138 score: 126.18  average score -28.86\n",
      "episode:  139 score: 171.39  average score -24.87\n",
      "episode:  140 score: -76.70  average score -23.29\n",
      "episode:  141 score: 15.10  average score -20.73\n",
      "episode:  142 score: 245.49  average score -17.43\n",
      "episode:  143 score: 135.58  average score -15.31\n",
      "episode:  144 score: 223.42  average score -11.18\n",
      "episode:  145 score: 256.90  average score -6.77\n",
      "episode:  146 score: 258.24  average score -3.69\n",
      "episode:  147 score: 227.78  average score -0.55\n",
      "episode:  148 score: 271.39  average score 3.23\n",
      "episode:  149 score: 259.65  average score 6.87\n",
      "episode:  150 score: 261.63  average score 11.14\n",
      "episode:  151 score: 238.32  average score 14.35\n",
      "episode:  152 score: 28.73  average score 15.20\n",
      "episode:  153 score: 249.38  average score 18.34\n",
      "episode:  154 score: 31.20  average score 19.26\n",
      "episode:  155 score: 2.25  average score 19.53\n",
      "episode:  156 score: 249.13  average score 22.88\n",
      "episode:  157 score: 301.33  average score 27.11\n",
      "episode:  158 score: 268.59  average score 30.41\n",
      "episode:  159 score: 213.86  average score 33.09\n",
      "episode:  160 score: 265.04  average score 36.04\n",
      "episode:  161 score: 216.61  average score 39.11\n",
      "episode:  162 score: -26.81  average score 39.53\n",
      "episode:  163 score: 230.48  average score 41.85\n",
      "episode:  164 score: -4.19  average score 41.91\n",
      "episode:  165 score: 3.03  average score 42.59\n",
      "episode:  166 score: 189.22  average score 44.96\n",
      "episode:  167 score: -0.86  average score 45.41\n",
      "episode:  168 score: 120.10  average score 47.61\n",
      "episode:  169 score: -58.71  average score 47.53\n",
      "episode:  170 score: -117.61  average score 47.04\n",
      "episode:  171 score: -65.02  average score 47.20\n",
      "episode:  172 score: 214.91  average score 49.68\n",
      "episode:  173 score: 219.17  average score 52.23\n",
      "episode:  174 score: 196.52  average score 54.64\n",
      "episode:  175 score: 201.62  average score 57.12\n",
      "episode:  176 score: 196.38  average score 61.54\n",
      "episode:  177 score: 191.81  average score 63.50\n",
      "episode:  178 score: 201.49  average score 65.62\n",
      "episode:  179 score: -130.27  average score 64.43\n",
      "episode:  180 score: -217.06  average score 62.42\n",
      "episode:  181 score: 217.60  average score 64.89\n",
      "episode:  182 score: -49.22  average score 64.43\n",
      "episode:  183 score: -96.78  average score 63.45\n",
      "episode:  184 score: -24.34  average score 63.76\n",
      "episode:  185 score: 230.84  average score 66.62\n",
      "episode:  186 score: 289.59  average score 69.97\n",
      "episode:  187 score: 229.36  average score 72.64\n",
      "episode:  188 score: 211.66  average score 75.40\n",
      "episode:  189 score: 255.65  average score 78.25\n",
      "episode:  190 score: 244.93  average score 81.00\n",
      "episode:  191 score: -39.83  average score 81.09\n",
      "episode:  192 score: 45.71  average score 81.73\n",
      "episode:  193 score: 252.28  average score 84.39\n",
      "episode:  194 score: 244.02  average score 87.42\n",
      "episode:  195 score: -46.10  average score 87.29\n",
      "episode:  196 score: 190.15  average score 91.06\n",
      "episode:  197 score: 228.71  average score 94.41\n",
      "episode:  198 score: 234.08  average score 97.62\n",
      "episode:  199 score: 252.71  average score 100.42\n",
      "episode:  200 score: 209.13  average score 103.14\n",
      "episode:  201 score: 267.21  average score 106.40\n",
      "episode:  202 score: -50.90  average score 105.83\n",
      "episode:  203 score: 185.67  average score 107.66\n",
      "episode:  204 score: 203.73  average score 110.98\n",
      "episode:  205 score: -114.98  average score 108.05\n",
      "episode:  206 score: 205.75  average score 108.47\n",
      "episode:  207 score: 264.98  average score 108.51\n",
      "episode:  208 score: 202.33  average score 110.27\n",
      "episode:  209 score: 219.60  average score 109.58\n",
      "episode:  210 score: 215.13  average score 109.57\n",
      "episode:  211 score: 247.58  average score 113.15\n",
      "episode:  212 score: 212.32  average score 113.02\n",
      "episode:  213 score: 219.95  average score 117.65\n",
      "episode:  214 score: 277.61  average score 119.10\n",
      "episode:  215 score: 179.63  average score 121.73\n",
      "episode:  216 score: 2.44  average score 123.15\n",
      "episode:  217 score: 165.35  average score 125.92\n",
      "episode:  218 score: 211.58  average score 126.64\n",
      "episode:  219 score: 273.54  average score 130.98\n",
      "episode:  220 score: 236.77  average score 132.39\n",
      "episode:  221 score: 264.20  average score 136.90\n",
      "episode:  222 score: 229.29  average score 140.89\n",
      "episode:  223 score: 252.55  average score 144.70\n",
      "episode:  224 score: 245.36  average score 146.41\n",
      "episode:  225 score: 232.11  average score 149.53\n",
      "episode:  226 score: 236.63  average score 152.73\n",
      "episode:  227 score: 273.24  average score 156.82\n",
      "episode:  228 score: 274.67  average score 159.80\n",
      "episode:  229 score: 242.81  average score 161.34\n",
      "episode:  230 score: 227.46  average score 160.86\n",
      "episode:  231 score: 233.51  average score 160.72\n",
      "episode:  232 score: 249.76  average score 160.58\n",
      "episode:  233 score: 238.01  average score 160.32\n",
      "episode:  234 score: -67.01  average score 157.11\n",
      "episode:  235 score: 200.04  average score 160.63\n",
      "episode:  236 score: 230.24  average score 163.35\n",
      "episode:  237 score: 225.94  average score 163.25\n",
      "episode:  238 score: 308.96  average score 164.24\n",
      "episode:  239 score: 28.77  average score 163.27\n",
      "episode:  240 score: -3.36  average score 161.54\n",
      "episode:  241 score: 234.35  average score 164.62\n",
      "episode:  242 score: 282.41  average score 167.27\n",
      "episode:  243 score: 154.64  average score 166.37\n",
      "episode:  244 score: 202.29  average score 167.03\n",
      "episode:  245 score: 183.11  average score 166.63\n",
      "episode:  246 score: 211.63  average score 166.18\n",
      "episode:  247 score: 285.41  average score 166.45\n",
      "episode:  248 score: 273.24  average score 166.90\n",
      "episode:  249 score: 289.43  average score 167.08\n",
      "episode:  250 score: 299.53  average score 167.47\n",
      "episode:  251 score: 206.50  average score 166.93\n",
      "episode:  252 score: 11.99  average score 164.69\n",
      "episode:  253 score: 265.06  average score 167.03\n",
      "episode:  254 score: -15.46  average score 164.41\n",
      "episode:  255 score: 144.89  average score 165.53\n",
      "episode:  256 score: 263.26  average score 168.12\n",
      "episode:  257 score: 251.70  average score 168.14\n",
      "episode:  258 score: 255.50  average score 167.69\n",
      "episode:  259 score: 55.60  average score 165.58\n",
      "episode:  260 score: 296.75  average score 166.40\n",
      "episode:  261 score: 255.74  average score 166.31\n",
      "episode:  262 score: 221.77  average score 166.36\n",
      "episode:  263 score: 232.41  average score 168.92\n",
      "episode:  264 score: 262.83  average score 169.24\n",
      "episode:  265 score: 260.01  average score 171.86\n",
      "episode:  266 score: 181.50  average score 173.63\n",
      "episode:  267 score: 270.04  average score 174.43\n",
      "episode:  268 score: 245.07  average score 176.86\n",
      "episode:  269 score: 265.23  average score 178.30\n",
      "episode:  270 score: 272.81  average score 181.58\n",
      "episode:  271 score: 271.48  average score 185.43\n",
      "episode:  272 score: 242.91  average score 188.48\n",
      "episode:  273 score: 251.18  average score 188.84\n",
      "episode:  274 score: 269.42  average score 189.34\n",
      "episode:  275 score: 30.00  average score 187.69\n",
      "episode:  276 score: 260.27  average score 188.27\n",
      "episode:  277 score: 273.32  average score 189.03\n",
      "episode:  278 score: 229.76  average score 189.41\n",
      "episode:  279 score: 282.96  average score 190.22\n",
      "episode:  280 score: 133.57  average score 192.83\n",
      "episode:  281 score: 260.16  average score 197.55\n",
      "episode:  282 score: 249.66  average score 197.87\n",
      "episode:  283 score: 272.67  average score 201.06\n",
      "episode:  284 score: 268.32  average score 204.67\n",
      "episode:  285 score: 6.53  average score 204.98\n",
      "episode:  286 score: 261.06  average score 205.28\n",
      "episode:  287 score: 264.14  average score 205.02\n",
      "episode:  288 score: 185.75  average score 204.59\n",
      "episode:  289 score: 244.01  average score 204.91\n",
      "episode:  290 score: 274.41  average score 205.10\n",
      "episode:  291 score: 292.47  average score 205.57\n",
      "episode:  292 score: 9.42  average score 206.06\n",
      "episode:  293 score: 286.39  average score 208.44\n",
      "episode:  294 score: 271.00  average score 208.63\n",
      "episode:  295 score: 292.92  average score 209.11\n",
      "episode:  296 score: 258.37  average score 212.12\n",
      "episode:  297 score: 42.55  average score 210.66\n",
      "episode:  298 score: 287.08  average score 211.24\n",
      "episode:  299 score: 310.31  average score 212.00\n",
      "episode:  300 score: 237.14  average score 211.84\n",
      "episode:  301 score: 25.53  average score 210.02\n",
      "episode:  302 score: 232.02  average score 209.68\n",
      "episode:  303 score: 296.16  average score 213.11\n",
      "episode:  304 score: 268.65  average score 213.93\n",
      "episode:  305 score: 48.13  average score 212.39\n",
      "episode:  306 score: 269.15  average score 216.20\n",
      "episode:  307 score: -0.90  average score 214.15\n",
      "episode:  308 score: 266.19  average score 214.16\n",
      "episode:  309 score: 9.22  average score 212.25\n",
      "episode:  310 score: 303.23  average score 213.08\n",
      "episode:  311 score: 263.38  average score 213.56\n",
      "episode:  312 score: 275.37  average score 213.83\n",
      "episode:  313 score: 241.24  average score 214.12\n",
      "episode:  314 score: 282.67  average score 214.74\n",
      "episode:  315 score: 12.29  average score 212.11\n",
      "episode:  316 score: 214.79  average score 212.46\n",
      "episode:  317 score: 220.03  average score 214.61\n",
      "episode:  318 score: 308.37  average score 216.03\n",
      "episode:  319 score: 263.99  average score 216.55\n",
      "episode:  320 score: 247.91  average score 216.30\n",
      "episode:  321 score: 247.35  average score 216.40\n",
      "episode:  322 score: 304.04  average score 216.79\n",
      "episode:  323 score: 257.19  average score 217.07\n",
      "episode:  324 score: 245.75  average score 217.00\n",
      "episode:  325 score: 265.46  average score 217.20\n",
      "episode:  326 score: 258.12  average score 217.46\n",
      "episode:  327 score: 277.69  average score 217.87\n",
      "episode:  328 score: 264.88  average score 217.78\n",
      "episode:  329 score: 273.68  average score 217.77\n",
      "episode:  330 score: 265.32  average score 218.00\n",
      "episode:  331 score: 303.58  average score 218.75\n",
      "episode:  332 score: 299.61  average score 219.40\n",
      "episode:  333 score: 256.83  average score 219.47\n",
      "episode:  334 score: 261.04  average score 219.70\n",
      "episode:  335 score: 49.09  average score 220.85\n",
      "episode:  336 score: 251.20  average score 221.36\n",
      "episode:  337 score: 277.16  average score 221.82\n",
      "episode:  338 score: 241.19  average score 221.97\n",
      "episode:  339 score: 217.89  average score 221.07\n",
      "episode:  340 score: 300.85  average score 223.77\n",
      "episode:  341 score: 140.27  average score 225.19\n",
      "episode:  342 score: 113.43  average score 223.99\n",
      "episode:  343 score: 244.88  average score 223.62\n",
      "episode:  344 score: 11.84  average score 222.21\n",
      "episode:  345 score: 215.24  average score 222.33\n",
      "episode:  346 score: 268.27  average score 223.18\n",
      "episode:  347 score: 267.40  average score 223.73\n",
      "episode:  348 score: 182.82  average score 222.71\n",
      "episode:  349 score: 13.91  average score 220.15\n",
      "episode:  350 score: 247.69  average score 219.73\n",
      "episode:  351 score: 288.70  average score 219.63\n",
      "episode:  352 score: 275.21  average score 220.31\n",
      "episode:  353 score: 250.16  average score 222.66\n",
      "episode:  354 score: 261.43  average score 222.63\n",
      "episode:  355 score: 276.36  average score 225.52\n",
      "episode:  356 score: 261.27  average score 226.67\n",
      "episode:  357 score: 37.56  average score 224.43\n",
      "episode:  358 score: 22.32  average score 222.16\n",
      "episode:  359 score: 275.28  average score 222.36\n",
      "episode:  360 score: 282.12  average score 224.60\n",
      "episode:  361 score: -28.80  average score 221.38\n",
      "episode:  362 score: 32.49  average score 219.17\n",
      "episode:  363 score: 41.59  average score 217.38\n",
      "episode:  364 score: 33.42  average score 215.41\n",
      "episode:  365 score: 259.72  average score 215.38\n",
      "episode:  366 score: 254.80  average score 215.33\n",
      "episode:  367 score: 227.12  average score 215.78\n",
      "episode:  368 score: 252.60  average score 215.61\n",
      "episode:  369 score: 275.06  average score 215.91\n",
      "episode:  370 score: 42.52  average score 213.70\n",
      "episode:  371 score: 246.72  average score 213.44\n",
      "episode:  372 score: 241.35  average score 213.15\n",
      "episode:  373 score: 286.74  average score 213.58\n",
      "episode:  374 score: 135.33  average score 212.43\n",
      "episode:  375 score: 273.88  average score 212.48\n",
      "episode:  376 score: 213.85  average score 214.30\n",
      "episode:  377 score: 207.46  average score 213.77\n",
      "episode:  378 score: 266.34  average score 213.71\n",
      "episode:  379 score: 205.29  average score 213.46\n",
      "episode:  380 score: 237.75  average score 213.02\n",
      "episode:  381 score: 88.86  average score 212.57\n",
      "episode:  382 score: 300.98  average score 212.98\n",
      "episode:  383 score: 205.38  average score 212.54\n",
      "episode:  384 score: 239.12  average score 212.21\n",
      "episode:  385 score: 230.68  average score 211.83\n",
      "episode:  386 score: 268.28  average score 214.43\n",
      "episode:  387 score: 297.24  average score 214.78\n",
      "episode:  388 score: 286.07  average score 215.00\n",
      "episode:  389 score: 264.36  average score 215.78\n",
      "episode:  390 score: 236.75  average score 215.71\n",
      "episode:  391 score: 317.55  average score 216.13\n",
      "episode:  392 score: 269.35  average score 215.91\n",
      "episode:  393 score: 247.91  average score 218.27\n",
      "episode:  394 score: 298.42  average score 218.39\n",
      "episode:  395 score: 314.07  average score 218.81\n",
      "episode:  396 score: 222.82  average score 218.12\n",
      "episode:  397 score: 231.37  average score 217.85\n",
      "episode:  398 score: 241.16  average score 219.82\n",
      "episode:  399 score: 264.35  average score 219.59\n",
      "episode:  400 score: 229.32  average score 218.79\n",
      "episode:  401 score: 276.66  average score 219.18\n",
      "episode:  402 score: 219.09  average score 221.10\n",
      "episode:  403 score: 268.91  average score 221.46\n",
      "episode:  404 score: 283.30  average score 221.34\n",
      "episode:  405 score: 251.77  average score 221.17\n",
      "episode:  406 score: 311.81  average score 223.78\n",
      "episode:  407 score: 275.83  average score 223.85\n",
      "episode:  408 score: 243.80  average score 226.27\n",
      "episode:  409 score: 276.30  average score 226.37\n",
      "episode:  410 score: 171.48  average score 227.98\n",
      "episode:  411 score: 166.71  average score 226.62\n",
      "episode:  412 score: 26.79  average score 224.28\n",
      "episode:  413 score: 184.91  average score 223.39\n",
      "episode:  414 score: 302.75  average score 223.99\n",
      "episode:  415 score: 269.16  average score 223.86\n",
      "episode:  416 score: -4.02  average score 223.70\n",
      "episode:  417 score: 244.46  average score 223.99\n",
      "episode:  418 score: 262.68  average score 224.42\n",
      "episode:  419 score: 282.40  average score 224.16\n",
      "episode:  420 score: 234.90  average score 223.87\n",
      "episode:  421 score: -3.67  average score 221.38\n",
      "episode:  422 score: 231.75  average score 221.22\n",
      "episode:  423 score: 285.38  average score 221.04\n",
      "episode:  424 score: 246.80  average score 220.94\n",
      "episode:  425 score: -17.47  average score 218.33\n",
      "episode:  426 score: 271.54  average score 218.39\n",
      "episode:  427 score: 248.60  average score 218.30\n",
      "episode:  428 score: 264.20  average score 218.16\n",
      "episode:  429 score: 261.90  average score 218.13\n",
      "episode:  430 score: 264.95  average score 218.05\n",
      "episode:  431 score: 17.03  average score 215.59\n",
      "episode:  432 score: -147.41  average score 211.12\n",
      "episode:  433 score: 147.94  average score 209.62\n",
      "episode:  434 score: 251.84  average score 209.57\n",
      "episode:  435 score: 174.39  average score 208.71\n",
      "episode:  436 score: 238.42  average score 210.59\n",
      "episode:  437 score: 121.54  average score 209.31\n",
      "episode:  438 score: 238.56  average score 208.92\n",
      "episode:  439 score: -28.22  average score 206.26\n",
      "episode:  440 score: 6.55  average score 204.16\n",
      "episode:  441 score: 124.34  average score 202.42\n",
      "episode:  442 score: -28.68  average score 200.74\n",
      "episode:  443 score: 270.03  average score 202.29\n",
      "episode:  444 score: 281.20  average score 202.65\n",
      "episode:  445 score: 273.04  average score 205.24\n",
      "episode:  446 score: 236.25  average score 205.45\n",
      "episode:  447 score: 243.36  average score 205.20\n",
      "episode:  448 score: 255.49  average score 205.08\n",
      "episode:  449 score: 233.54  average score 205.59\n",
      "episode:  450 score: 211.18  average score 207.54\n",
      "episode:  451 score: 229.54  average score 207.36\n",
      "episode:  452 score: 249.75  average score 206.97\n",
      "episode:  453 score: 287.31  average score 207.09\n",
      "episode:  454 score: 306.75  average score 207.65\n",
      "episode:  455 score: 243.94  average score 207.48\n",
      "episode:  456 score: 253.25  average score 207.25\n",
      "episode:  457 score: 256.42  average score 207.20\n",
      "episode:  458 score: 246.85  average score 209.28\n",
      "episode:  459 score: 216.31  average score 211.20\n",
      "episode:  460 score: 271.41  average score 211.16\n",
      "episode:  461 score: 245.73  average score 210.80\n",
      "episode:  462 score: 244.97  average score 213.51\n",
      "episode:  463 score: 35.29  average score 213.54\n",
      "episode:  464 score: 197.55  average score 215.08\n",
      "episode:  465 score: 251.82  average score 217.24\n",
      "episode:  466 score: 267.85  average score 217.32\n",
      "episode:  467 score: 231.84  average score 217.10\n",
      "episode:  468 score: 237.27  average score 217.20\n",
      "episode:  469 score: 269.51  average score 217.36\n",
      "episode:  470 score: 260.63  average score 217.22\n",
      "episode:  471 score: 240.28  average score 219.18\n",
      "episode:  472 score: 247.03  average score 219.18\n",
      "episode:  473 score: 259.46  average score 219.36\n",
      "episode:  474 score: -93.55  average score 215.60\n",
      "episode:  475 score: 262.67  average score 216.86\n",
      "episode:  476 score: 253.05  average score 216.65\n",
      "episode:  477 score: 242.04  average score 216.93\n",
      "episode:  478 score: 266.61  average score 217.52\n",
      "episode:  479 score: -72.23  average score 214.16\n",
      "episode:  480 score: 271.24  average score 214.82\n",
      "episode:  481 score: 307.71  average score 215.51\n",
      "episode:  482 score: -28.76  average score 214.34\n",
      "episode:  483 score: 295.76  average score 214.29\n",
      "episode:  484 score: 289.94  average score 215.13\n",
      "episode:  485 score: 295.43  average score 215.69\n",
      "episode:  486 score: -96.67  average score 212.45\n",
      "episode:  487 score: 281.50  average score 212.58\n",
      "episode:  488 score: 248.39  average score 212.09\n",
      "episode:  489 score: 244.94  average score 211.69\n",
      "episode:  490 score: -36.59  average score 208.71\n",
      "episode:  491 score: 291.45  average score 209.25\n",
      "episode:  492 score: 290.10  average score 208.98\n",
      "episode:  493 score: 257.28  average score 208.86\n",
      "episode:  494 score: 220.56  average score 208.59\n",
      "episode:  495 score: 203.37  average score 207.65\n",
      "episode:  496 score: 44.37  average score 204.98\n",
      "episode:  497 score: 265.40  average score 205.40\n",
      "episode:  498 score: 13.26  average score 203.24\n",
      "episode:  499 score: 274.84  average score 203.57\n",
      "episode:  500 score: 261.15  average score 203.54\n"
     ]
    }
   ],
   "source": [
    "epochs = 501\n",
    "rewards_history = []\n",
    "eps_history = []\n",
    "logger = FileLogger('DDQN_History.log')\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = ddqn_agent.choose_action(observation)\n",
    "        observation2, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        ddqn_agent.remember(observation, action, reward, observation2, int(done))\n",
    "        observation = observation2\n",
    "        ddqn_agent.learn()\n",
    "    eps_history.append(ddqn_agent.epsilon)\n",
    "    rewards_history.append(score)\n",
    "\n",
    "    avg_score = np.mean(rewards_history[max(0, i-100):(i+1)])\n",
    "    print('episode: ', i,'score: %.2f' % score,\n",
    "            ' average score %.2f' % avg_score)\n",
    "    logger.log(i, score, avg_score)\n",
    "    if i % 5 == 0 and i > 0:\n",
    "        ddqn_agent.save_model(f'Models/DDQN/episode{i}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating DDQN\n",
    "From the graph below we can see that the moving average of the reward increases from episode 0 to 350. The average rewards starts to plateau after that.\n",
    "\n",
    "We choose the final models to test based on the peaks of the moving average line between 300 to 500 episodes. We tried visualizing lunar lander landing using the models from episode 350 to 360, 400 to 415 and 470 to 480.\n",
    "\n",
    "We found that the model that achieved a constand score above 200 was the model at 475 episodes. Hence, we chose that model as our final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andric\\anaconda3\\envs\\Reinforcement\\lib\\site-packages\\plotly\\tools.py:461: DeprecationWarning:\n",
      "\n",
      "plotly.tools.make_subplots is deprecated, please use plotly.subplots.make_subplots instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "Green"
         },
         "mode": "markers+lines",
         "name": "Avg Reward",
         "text": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48",
          "49",
          "50",
          "51",
          "52",
          "53",
          "54",
          "55",
          "56",
          "57",
          "58",
          "59",
          "60",
          "61",
          "62",
          "63",
          "64",
          "65",
          "66",
          "67",
          "68",
          "69",
          "70",
          "71",
          "72",
          "73",
          "74",
          "75",
          "76",
          "77",
          "78",
          "79",
          "80",
          "81",
          "82",
          "83",
          "84",
          "85",
          "86",
          "87",
          "88",
          "89",
          "90",
          "91",
          "92",
          "93",
          "94",
          "95",
          "96",
          "97",
          "98",
          "99",
          "100",
          "101",
          "102",
          "103",
          "104",
          "105",
          "106",
          "107",
          "108",
          "109",
          "110",
          "111",
          "112",
          "113",
          "114",
          "115",
          "116",
          "117",
          "118",
          "119",
          "120",
          "121",
          "122",
          "123",
          "124",
          "125",
          "126",
          "127",
          "128",
          "129",
          "130",
          "131",
          "132",
          "133",
          "134",
          "135",
          "136",
          "137",
          "138",
          "139",
          "140",
          "141",
          "142",
          "143",
          "144",
          "145",
          "146",
          "147",
          "148",
          "149",
          "150",
          "151",
          "152",
          "153",
          "154",
          "155",
          "156",
          "157",
          "158",
          "159",
          "160",
          "161",
          "162",
          "163",
          "164",
          "165",
          "166",
          "167",
          "168",
          "169",
          "170",
          "171",
          "172",
          "173",
          "174",
          "175",
          "176",
          "177",
          "178",
          "179",
          "180",
          "181",
          "182",
          "183",
          "184",
          "185",
          "186",
          "187",
          "188",
          "189",
          "190",
          "191",
          "192",
          "193",
          "194",
          "195",
          "196",
          "197",
          "198",
          "199",
          "200",
          "201",
          "202",
          "203",
          "204",
          "205",
          "206",
          "207",
          "208",
          "209",
          "210",
          "211",
          "212",
          "213",
          "214",
          "215",
          "216",
          "217",
          "218",
          "219",
          "220",
          "221",
          "222",
          "223",
          "224",
          "225",
          "226",
          "227",
          "228",
          "229",
          "230",
          "231",
          "232",
          "233",
          "234",
          "235",
          "236",
          "237",
          "238",
          "239",
          "240",
          "241",
          "242",
          "243",
          "244",
          "245",
          "246",
          "247",
          "248",
          "249",
          "250",
          "251",
          "252",
          "253",
          "254",
          "255",
          "256",
          "257",
          "258",
          "259",
          "260",
          "261",
          "262",
          "263",
          "264",
          "265",
          "266",
          "267",
          "268",
          "269",
          "270",
          "271",
          "272",
          "273",
          "274",
          "275",
          "276",
          "277",
          "278",
          "279",
          "280",
          "281",
          "282",
          "283",
          "284",
          "285",
          "286",
          "287",
          "288",
          "289",
          "290",
          "291",
          "292",
          "293",
          "294",
          "295",
          "296",
          "297",
          "298",
          "299",
          "300",
          "301",
          "302",
          "303",
          "304",
          "305",
          "306",
          "307",
          "308",
          "309",
          "310",
          "311",
          "312",
          "313",
          "314",
          "315",
          "316",
          "317",
          "318",
          "319",
          "320",
          "321",
          "322",
          "323",
          "324",
          "325",
          "326",
          "327",
          "328",
          "329",
          "330",
          "331",
          "332",
          "333",
          "334",
          "335",
          "336",
          "337",
          "338",
          "339",
          "340",
          "341",
          "342",
          "343",
          "344",
          "345",
          "346",
          "347",
          "348",
          "349",
          "350",
          "351",
          "352",
          "353",
          "354",
          "355",
          "356",
          "357",
          "358",
          "359",
          "360",
          "361",
          "362",
          "363",
          "364",
          "365",
          "366",
          "367",
          "368",
          "369",
          "370",
          "371",
          "372",
          "373",
          "374",
          "375",
          "376",
          "377",
          "378",
          "379",
          "380",
          "381",
          "382",
          "383",
          "384",
          "385",
          "386",
          "387",
          "388",
          "389",
          "390",
          "391",
          "392",
          "393",
          "394",
          "395",
          "396",
          "397",
          "398",
          "399",
          "400",
          "401",
          "402",
          "403",
          "404",
          "405",
          "406",
          "407",
          "408",
          "409",
          "410",
          "411",
          "412",
          "413",
          "414",
          "415",
          "416",
          "417",
          "418",
          "419",
          "420",
          "421",
          "422",
          "423",
          "424",
          "425",
          "426",
          "427",
          "428",
          "429",
          "430",
          "431",
          "432",
          "433",
          "434",
          "435",
          "436",
          "437",
          "438",
          "439",
          "440",
          "441",
          "442",
          "443",
          "444",
          "445",
          "446",
          "447",
          "448",
          "449",
          "450",
          "451",
          "452",
          "453",
          "454",
          "455",
          "456",
          "457",
          "458",
          "459",
          "460",
          "461",
          "462",
          "463",
          "464",
          "465",
          "466",
          "467",
          "468",
          "469",
          "470",
          "471",
          "472",
          "473",
          "474",
          "475",
          "476",
          "477",
          "478",
          "479",
          "480",
          "481",
          "482",
          "483",
          "484",
          "485",
          "486",
          "487",
          "488",
          "489",
          "490",
          "491",
          "492",
          "493",
          "494",
          "495",
          "496",
          "497",
          "498",
          "499",
          "500",
          "501"
         ],
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501
         ],
         "xaxis": "x",
         "y": [
          -137.52410325959008,
          -266.7147919688844,
          -280.62702912001964,
          -207.48742034200205,
          -166.58112097210258,
          -155.387071607237,
          -163.3580340249488,
          -167.820281403886,
          -180.30213603423476,
          -176.64662973605604,
          -176.93491714840792,
          -176.76043748689096,
          -179.60798085741695,
          -181.52524708018,
          -177.7378595871478,
          -170.82174075110697,
          -166.0518375570774,
          -156.48665179411375,
          -150.6843440983276,
          -147.87661277845922,
          -143.5409428085521,
          -141.42449838270483,
          -144.3408190600563,
          -141.28955006950707,
          -140.13120165398175,
          -138.35608041256552,
          -138.47057434150773,
          -136.32311366855092,
          -135.6848485591226,
          -134.7909483356373,
          -133.32377749190567,
          -132.39518961458356,
          -130.8736956104752,
          -134.22325331517,
          -133.9853581221987,
          -132.8891162250778,
          -149.73441003965746,
          -152.78916389893803,
          -154.80661819225386,
          -156.8374500126872,
          -158.95630436748593,
          -157.27039529584056,
          -155.42701111428676,
          -156.3049873492903,
          -157.02593889873262,
          -154.76020947368525,
          -153.35503690565608,
          -152.47630959078137,
          -151.55472905521694,
          -151.92228019164534,
          -150.63060047751196,
          -148.8352693414662,
          -147.30808420317726,
          -145.7133358971622,
          -143.52064075793757,
          -142.54749990806522,
          -142.2499585506359,
          -140.91568267695814,
          -139.48930496369653,
          -137.7172992359676,
          -136.9901970402954,
          -135.8933563002521,
          -133.79876620848984,
          -131.8703870719577,
          -130.86102894268373,
          -129.639644768162,
          -128.39174058022252,
          -128.00596766998794,
          -126.88635711390371,
          -126.04249415352848,
          -125.40563176762596,
          -124.15542984131636,
          -122.9923740199018,
          -121.9560824786776,
          -120.98024632072136,
          -122.67729421648392,
          -121.16703433425128,
          -119.78249378706091,
          -118.39116289466097,
          -117.08285198752672,
          -116.0342983748457,
          -114.65088341418716,
          -113.23949629552313,
          -112.5555574357268,
          -111.9155606808348,
          -111.18455551422066,
          -110.3613374134287,
          -109.87615393504662,
          -109.00454764230147,
          -108.1502588138888,
          -107.5081011491524,
          -106.54638817250088,
          -105.57546133257738,
          -105.10912964486134,
          -104.35088465282794,
          -105.24819087453425,
          -105.28858758209924,
          -105.13999440095137,
          -104.3824843128384,
          -103.99461020113014,
          -103.57567860903504,
          -102.14625283215248,
          -98.2224466083134,
          -96.4665409891278,
          -94.79212748469196,
          -93.14942868696733,
          -89.58742309159997,
          -87.2458731651871,
          -82.40930479668357,
          -77.49880304414643,
          -77.20133182217654,
          -73.19290783693802,
          -73.9145363712978,
          -70.4962366514164,
          -69.30245451947441,
          -69.47048462912699,
          -69.93985379614584,
          -67.67533668333138,
          -69.3611711427811,
          -67.9714504129097,
          -68.93118713421683,
          -70.08949466179472,
          -70.43347532293527,
          -67.64926727432008,
          -67.77082858593732,
          -67.52092336981605,
          -67.97267012623324,
          -66.82749993415577,
          -65.1915116313283,
          -61.29269060650158,
          -57.76763244100948,
          -54.26199313178629,
          -50.62048111533456,
          -47.26177858450494,
          -46.38075713839262,
          -45.570395064069,
          -42.29764417484138,
          -32.74066443853916,
          -28.85956189871826,
          -24.87088879346977,
          -23.29328654659338,
          -20.730772677614,
          -17.427431907229586,
          -15.312687031312118,
          -11.179201669182692,
          -6.766878098347321,
          -3.6872696797072377,
          -0.5535976163369228,
          3.2341748406944477,
          6.86748834843682,
          11.140341279843208,
          14.35186166948521,
          15.20333813445719,
          18.34471077313501,
          19.259471073679222,
          19.53036877315582,
          22.87843225575297,
          27.105337650467995,
          30.40687873847141,
          33.08627168885454,
          36.03884682530485,
          39.10794368341225,
          39.52552278990958,
          41.8464391429351,
          41.907714700588734,
          42.59378244394002,
          44.96480768887844,
          45.41207654786444,
          47.61268533031044,
          47.53390534145959,
          47.04086174259497,
          47.19733729142133,
          49.67556360530679,
          52.23423178592202,
          54.63848085374589,
          57.117564002944206,
          61.53673098242796,
          63.49907221865645,
          65.62444219031495,
          64.43230859191901,
          62.41915085463125,
          64.8918923856687,
          64.43025948896769,
          63.44737440537664,
          63.75876544817236,
          66.62007567782325,
          69.97297147803192,
          72.63557131170859,
          75.40119098188494,
          78.25225778683439,
          80.99527211825567,
          81.0931601597426,
          81.73414243583284,
          84.39289779321734,
          87.42027246179245,
          87.29130839585788,
          91.0600669572286,
          94.40535481710538,
          97.62128344192966,
          100.42183219903596,
          103.14190111454302,
          106.39826638047292,
          105.82652173765511,
          107.66084064004608,
          110.97598993182108,
          108.0450097125357,
          108.46872258417456,
          108.51456398106642,
          110.26724617293964,
          109.57573768458673,
          109.56903824670262,
          113.1461248570156,
          113.02029710340244,
          117.65071981612792,
          119.0976109873848,
          121.72641220078624,
          123.15339073048834,
          125.92400638737227,
          126.64276504416998,
          130.97627419255758,
          132.38863831765957,
          136.9001356782592,
          140.8912496777069,
          144.6959383677254,
          146.40537732222109,
          149.52912197011813,
          152.73427995270828,
          156.821847198905,
          159.79664903284856,
          161.34039655206635,
          160.8601407195025,
          160.72499213153614,
          160.57646034748925,
          160.31727704270892,
          157.10881067421715,
          160.63169460555034,
          163.34742482295638,
          163.24750968700954,
          164.2362995550153,
          163.27189908438191,
          161.54169299733005,
          164.62142318387268,
          167.2679990074825,
          166.36847222711515,
          167.0289450499901,
          166.62983375883437,
          166.18168739065757,
          166.45073102704646,
          166.90078054856227,
          167.07941794076703,
          167.4742986374765,
          166.9285097636506,
          164.68763553076755,
          167.02753088878376,
          164.40527491799295,
          165.53088145743698,
          168.1152192735287,
          168.1406324086325,
          167.68686194815055,
          165.5780070029109,
          166.39871044462527,
          166.30665641719966,
          166.35771354740737,
          168.92425653278474,
          169.2445442221003,
          171.8603935053453,
          173.6273939034385,
          174.4275226022772,
          176.86240697908957,
          178.29933842883605,
          181.5816836618706,
          185.43414184098097,
          188.48294646868288,
          188.842038789173,
          189.33949165439716,
          187.6907450778559,
          188.2714407545019,
          189.03323928394173,
          189.4089523228789,
          190.2155460248098,
          192.8278812281991,
          197.55273809310867,
          197.8701731441816,
          201.05715357920857,
          204.672000118508,
          204.9776691358969,
          205.2769241282239,
          205.0249576091886,
          204.5932380430437,
          204.913481200866,
          205.09922076803736,
          205.56996354350912,
          206.0575784032484,
          208.4405290674456,
          208.6258374035056,
          209.1099984849372,
          212.12454960136807,
          210.6631425385816,
          211.24108962411427,
          211.9958344767082,
          211.841656602004,
          210.02380176912996,
          209.6754297378435,
          213.11169656632305,
          213.933289105058,
          212.3927661002086,
          216.19602685468573,
          214.15000738328973,
          214.16202748035505,
          212.25000565749724,
          213.07804297967604,
          213.55575920141263,
          213.8309067330452,
          214.1171634542419,
          214.73818913370025,
          212.1113172247838,
          212.45938993401865,
          214.613794539708,
          216.029860892581,
          216.5487643527733,
          216.29502016817463,
          216.3998065315004,
          216.79429354361343,
          217.0705873350306,
          217.0032732914593,
          217.2023476710006,
          217.4598536105769,
          217.86634482344573,
          217.78354955594696,
          217.77375610980687,
          217.99661417922968,
          218.7502787695132,
          219.4046864236649,
          219.4746497442217,
          219.7026916141845,
          220.8521769746493,
          221.3587304890904,
          221.82330279512016,
          221.9743478580969,
          221.0726822257162,
          223.76652517105128,
          225.18861608153287,
          223.99134734035204,
          223.6197465656413,
          222.2058692293872,
          222.3340861220368,
          223.17719809671377,
          223.729362784677,
          222.7135758130007,
          220.1459551131464,
          219.7327189082125,
          219.6254599989009,
          220.30570157579373,
          222.66382159229704,
          222.6279032042803,
          225.51722003336255,
          226.66952124180384,
          224.43482431254924,
          222.16372629770964,
          222.35954226593725,
          224.60233275901297,
          221.3790405680274,
          219.1685609025285,
          217.3846151999034,
          215.41439586398468,
          215.3836530309501,
          215.3320781746761,
          215.7837979086614,
          215.6111337550524,
          215.90810976051552,
          213.7030343973241,
          213.44476811175693,
          213.1463869711482,
          213.5804121943252,
          212.43336132601348,
          212.47753243107988,
          214.2978331920178,
          213.77498613981248,
          213.70585884764876,
          213.46364097182192,
          213.01602476023763,
          212.57327484770732,
          212.97751302170607,
          212.5391197533908,
          212.206996419986,
          211.83426782869688,
          214.42579835012668,
          214.78398978200764,
          215.00110120780883,
          215.7794211196716,
          215.70759817454652,
          216.1347097595928,
          215.90577264040192,
          218.26708300806487,
          218.3862060075257,
          218.81264135963775,
          218.118485865936,
          217.851131382003,
          219.8175639203365,
          219.59253880138303,
          218.7906485607416,
          219.18200570242183,
          221.098503689542,
          221.4636947786813,
          221.33632146141167,
          221.1692283437155,
          223.77986372912267,
          223.84600262732584,
          226.2687300291541,
          226.3688249156906,
          227.97532073629267,
          226.62366264729283,
          224.28117433810235,
          223.3855229884078,
          223.9945649736337,
          223.8607553750637,
          223.6992589396418,
          223.99309744213116,
          224.41536566253973,
          224.15828919355965,
          223.87031977720824,
          221.3794255233053,
          221.22498245963587,
          221.040231114721,
          220.93734089548605,
          218.3311301463924,
          218.3913230478408,
          218.2970737421397,
          218.1635016594749,
          218.134031562266,
          218.0476222386367,
          215.58925595372256,
          211.12396761292172,
          209.62226828483043,
          209.5728726749604,
          208.71496684707904,
          210.58957320529197,
          209.3058104246361,
          208.92363641551384,
          206.25623066774725,
          204.1637811642214,
          202.4162045461373,
          200.74346058430137,
          202.2939943846965,
          202.6536845412243,
          205.23988362597112,
          205.4478893047701,
          205.2012964041477,
          205.08334633295703,
          205.58556678593047,
          207.53878958602053,
          207.35902260305323,
          206.97345799690743,
          207.0932686354909,
          207.6536001055753,
          207.4804985286317,
          207.25168448739024,
          207.20363765708373,
          209.2758718929476,
          211.19661743903916,
          211.15836796695936,
          210.79808196257665,
          213.50869319223824,
          213.53648015380577,
          215.0805631468405,
          217.24294428292723,
          217.32341806585328,
          217.09608693091585,
          217.1965954215807,
          217.36399554121,
          217.2211203652459,
          219.1791559529684,
          219.18222940120663,
          219.36154807180785,
          215.59623144135227,
          216.85707409502209,
          216.65085052157968,
          216.92999742342093,
          217.5156317449496,
          214.16349877438864,
          214.8164638853717,
          215.50920265672477,
          214.3446595864694,
          214.2929325763155,
          215.13017141244964,
          215.68772548577425,
          212.44671033826887,
          212.5776150281517,
          212.0939335454545,
          211.68671907408608,
          208.70695350127767,
          209.2485010717557,
          208.97670259929637,
          208.85722151530211,
          208.58641027099924,
          207.64532047288384,
          204.9750235203744,
          205.3966956987844,
          203.23722075930056,
          203.5707032768242,
          203.5390071068366
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "Red"
         },
         "mode": "markers+lines",
         "name": "Reward",
         "text": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48",
          "49",
          "50",
          "51",
          "52",
          "53",
          "54",
          "55",
          "56",
          "57",
          "58",
          "59",
          "60",
          "61",
          "62",
          "63",
          "64",
          "65",
          "66",
          "67",
          "68",
          "69",
          "70",
          "71",
          "72",
          "73",
          "74",
          "75",
          "76",
          "77",
          "78",
          "79",
          "80",
          "81",
          "82",
          "83",
          "84",
          "85",
          "86",
          "87",
          "88",
          "89",
          "90",
          "91",
          "92",
          "93",
          "94",
          "95",
          "96",
          "97",
          "98",
          "99",
          "100",
          "101",
          "102",
          "103",
          "104",
          "105",
          "106",
          "107",
          "108",
          "109",
          "110",
          "111",
          "112",
          "113",
          "114",
          "115",
          "116",
          "117",
          "118",
          "119",
          "120",
          "121",
          "122",
          "123",
          "124",
          "125",
          "126",
          "127",
          "128",
          "129",
          "130",
          "131",
          "132",
          "133",
          "134",
          "135",
          "136",
          "137",
          "138",
          "139",
          "140",
          "141",
          "142",
          "143",
          "144",
          "145",
          "146",
          "147",
          "148",
          "149",
          "150",
          "151",
          "152",
          "153",
          "154",
          "155",
          "156",
          "157",
          "158",
          "159",
          "160",
          "161",
          "162",
          "163",
          "164",
          "165",
          "166",
          "167",
          "168",
          "169",
          "170",
          "171",
          "172",
          "173",
          "174",
          "175",
          "176",
          "177",
          "178",
          "179",
          "180",
          "181",
          "182",
          "183",
          "184",
          "185",
          "186",
          "187",
          "188",
          "189",
          "190",
          "191",
          "192",
          "193",
          "194",
          "195",
          "196",
          "197",
          "198",
          "199",
          "200",
          "201",
          "202",
          "203",
          "204",
          "205",
          "206",
          "207",
          "208",
          "209",
          "210",
          "211",
          "212",
          "213",
          "214",
          "215",
          "216",
          "217",
          "218",
          "219",
          "220",
          "221",
          "222",
          "223",
          "224",
          "225",
          "226",
          "227",
          "228",
          "229",
          "230",
          "231",
          "232",
          "233",
          "234",
          "235",
          "236",
          "237",
          "238",
          "239",
          "240",
          "241",
          "242",
          "243",
          "244",
          "245",
          "246",
          "247",
          "248",
          "249",
          "250",
          "251",
          "252",
          "253",
          "254",
          "255",
          "256",
          "257",
          "258",
          "259",
          "260",
          "261",
          "262",
          "263",
          "264",
          "265",
          "266",
          "267",
          "268",
          "269",
          "270",
          "271",
          "272",
          "273",
          "274",
          "275",
          "276",
          "277",
          "278",
          "279",
          "280",
          "281",
          "282",
          "283",
          "284",
          "285",
          "286",
          "287",
          "288",
          "289",
          "290",
          "291",
          "292",
          "293",
          "294",
          "295",
          "296",
          "297",
          "298",
          "299",
          "300",
          "301",
          "302",
          "303",
          "304",
          "305",
          "306",
          "307",
          "308",
          "309",
          "310",
          "311",
          "312",
          "313",
          "314",
          "315",
          "316",
          "317",
          "318",
          "319",
          "320",
          "321",
          "322",
          "323",
          "324",
          "325",
          "326",
          "327",
          "328",
          "329",
          "330",
          "331",
          "332",
          "333",
          "334",
          "335",
          "336",
          "337",
          "338",
          "339",
          "340",
          "341",
          "342",
          "343",
          "344",
          "345",
          "346",
          "347",
          "348",
          "349",
          "350",
          "351",
          "352",
          "353",
          "354",
          "355",
          "356",
          "357",
          "358",
          "359",
          "360",
          "361",
          "362",
          "363",
          "364",
          "365",
          "366",
          "367",
          "368",
          "369",
          "370",
          "371",
          "372",
          "373",
          "374",
          "375",
          "376",
          "377",
          "378",
          "379",
          "380",
          "381",
          "382",
          "383",
          "384",
          "385",
          "386",
          "387",
          "388",
          "389",
          "390",
          "391",
          "392",
          "393",
          "394",
          "395",
          "396",
          "397",
          "398",
          "399",
          "400",
          "401",
          "402",
          "403",
          "404",
          "405",
          "406",
          "407",
          "408",
          "409",
          "410",
          "411",
          "412",
          "413",
          "414",
          "415",
          "416",
          "417",
          "418",
          "419",
          "420",
          "421",
          "422",
          "423",
          "424",
          "425",
          "426",
          "427",
          "428",
          "429",
          "430",
          "431",
          "432",
          "433",
          "434",
          "435",
          "436",
          "437",
          "438",
          "439",
          "440",
          "441",
          "442",
          "443",
          "444",
          "445",
          "446",
          "447",
          "448",
          "449",
          "450",
          "451",
          "452",
          "453",
          "454",
          "455",
          "456",
          "457",
          "458",
          "459",
          "460",
          "461",
          "462",
          "463",
          "464",
          "465",
          "466",
          "467",
          "468",
          "469",
          "470",
          "471",
          "472",
          "473",
          "474",
          "475",
          "476",
          "477",
          "478",
          "479",
          "480",
          "481",
          "482",
          "483",
          "484",
          "485",
          "486",
          "487",
          "488",
          "489",
          "490",
          "491",
          "492",
          "493",
          "494",
          "495",
          "496",
          "497",
          "498",
          "499",
          "500",
          "501"
         ],
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501
         ],
         "xaxis": "x",
         "y": [
          -137.52410325959008,
          -395.9054806781787,
          -308.4515034222901,
          11.931405992050712,
          -2.9559234925047093,
          -99.41682478290912,
          -211.18380853121948,
          -199.05601305644643,
          -280.1569730770248,
          -143.74707305244775,
          -179.81779127192658,
          -174.84116121020446,
          -213.77850130372852,
          -206.4497079760997,
          -124.71443468469732,
          -67.07995821049444,
          -89.73338645260408,
          6.121506176268014,
          -46.242805574176856,
          -94.52971770096018,
          -56.82754341040994,
          -96.9791654399124,
          -208.49987396178832,
          -71.11036328687462,
          -112.33083968137385,
          -93.97804937716,
          -141.44741649400572,
          -78.34167549871651,
          -117.81342549512944,
          -108.86784185456318,
          -89.30865217995748,
          -103.608965417598,
          -82.18588747900804,
          -244.75865757009856,
          -125.89692156117384,
          -94.52064982584588,
          -756.1649873645258,
          -265.8150566923189,
          -231.4698813382556,
          -236.03989100958765,
          -243.71047855943505,
          -88.14812335838005,
          -78.0048754890265,
          -194.0579654544429,
          -188.74780707419515,
          -52.8023853465538,
          -88.71709877631342,
          -111.17612579167064,
          -107.31886334812366,
          -169.93228587663756,
          -86.04661477084353,
          -57.27338140313125,
          -67.89445701215261,
          -61.19167567836365,
          -25.115103239807397,
          -89.02475316508705,
          -125.58764253459408,
          -64.86195787732468,
          -56.75939759452421,
          -33.16896129996224,
          -93.36406529996148,
          -68.98607115760873,
          -3.934180519230583,
          -10.382501470434988,
          -66.26210866914839,
          -50.2496734242513,
          -46.03006417621692,
          -102.15918268427004,
          -50.75283930017682,
          -67.81594988763533,
          -80.82526475444972,
          -35.391093073335114,
          -39.252354878051726,
          -46.30679996931291,
          -48.76837063195987,
          -249.95588639867503,
          -6.387283284571054,
          -13.172871653401437,
          -9.8673532874637,
          -13.72629032392429,
          -32.15000936036472,
          -2.594271600844505,
          2.494247434928766,
          -55.78863207263341,
          -58.1558332699057,
          -49.049116352018885,
          -39.564580745319546,
          -67.66519131580917,
          -32.3031938807307,
          -32.11855308516001,
          -49.71391132287586,
          -19.030507297214168,
          -16.250192059615134,
          -61.74028268726946,
          -33.075855401687846,
          -190.49228193663004,
          -109.1666715083409,
          -90.72645582960534,
          -30.146495677766524,
          -65.59507314201475,
          -61.68251939952663,
          6.84790020554775,
          0.3989479295713776,
          -131.10503588454338,
          181.0471699400732,
          162.95665507768211,
          260.345740349193,
          25.31273403647934,
          289.43739216241033,
          215.80370392922657,
          -113.70247963349024,
          225.033031237166,
          -247.72564318054413,
          131.46977040429368,
          -85.87771264995834,
          -141.68547575960744,
          -114.48624407939823,
          138.98284194165603,
          -164.14777422815396,
          94.11898814283448,
          -191.4631265529796,
          -173.81660369577708,
          -131.72121221510827,
          72.70513894834548,
          -83.38805576021616,
          -87.09041285312455,
          -139.60447177529653,
          -25.78522709418157,
          86.89314308685756,
          275.967498012369,
          247.16303286013977,
          264.7609180515845,
          264.183748244027,
          257.0430681347835,
          -155.77549151275477,
          -44.05035205448867,
          236.0271899861449,
          209.08996600199688,
          126.17629982959244,
          171.38610229184138,
          -76.70206407507132,
          15.103422207481614,
          245.48929445044632,
          135.58435697863789,
          223.4240561206293,
          256.89687358017704,
          258.2380649360946,
          227.7837796240885,
          271.38889236849764,
          259.64580093385587,
          261.62586019540777,
          238.31694458299847,
          28.72574155903888,
          249.38417949430743,
          31.199114676601734,
          2.2455644073292405,
          249.12965857722463,
          301.32980233162345,
          268.5936920110201,
          213.85929039417232,
          265.04112748151795,
          216.6147173688876,
          -26.810581401379405,
          230.47837113634736,
          -4.193670147418659,
          3.0307334093325204,
          189.22387631452924,
          -0.8559094186314666,
          120.10230434277616,
          -58.70961817411349,
          -117.61335337296276,
          -65.02123432298713,
          214.90976462909637,
          219.17313136408703,
          196.52235588089792,
          201.6190274370713,
          196.37997852918448,
          191.8091815745049,
          201.48949548410909,
          -130.2728467254559,
          -217.05522178998865,
          217.5968852744184,
          -49.21919416764774,
          -96.777146007766,
          -24.33813675026495,
          230.83649992483492,
          289.59335946905367,
          229.35800245602545,
          211.662395372002,
          255.6545534191644,
          244.9258943883897,
          -39.82721913269578,
          45.70870258789978,
          252.28409903622077,
          244.02455883881663,
          -46.10122606107933,
          190.15233276181027,
          228.70740233921344,
          234.0823352776466,
          252.70892878997148,
          209.13188732419783,
          267.21037245939385,
          -50.89830871904856,
          185.66515707105697,
          203.7250425847324,
          -114.98183220774916,
          205.751655113207,
          264.9757214352692,
          202.333635415677,
          219.59503483876443,
          215.1270607029308,
          247.5832680081217,
          212.32442812223485,
          219.94705080472835,
          277.6057787012396,
          179.63120990358755,
          2.4393557403055386,
          165.3459372658798,
          211.57746627822505,
          273.53664975899534,
          236.7677647781356,
          264.1981068675813,
          229.28591024844127,
          252.55234547676025,
          245.3584733524078,
          232.11015367738244,
          236.6305433884806,
          273.23982009057573,
          274.6697581341156,
          242.8116425278558,
          227.46165892342117,
          233.5130254755369,
          249.75920786284797,
          238.0062344612156,
          -67.0120350828852,
          200.035785551896,
          230.23839990352323,
          225.93576125551056,
          308.9577426705814,
          28.77185229562167,
          -3.364712500400941,
          234.35068476573537,
          282.4075803920754,
          154.63708963333835,
          202.2921120890121,
          183.1138157139008,
          211.63409039431977,
          285.4114722113695,
          273.2387812971861,
          289.43126898118066,
          299.52875130150744,
          206.5011839389956,
          11.988647061810369,
          265.0551727186771,
          -15.463673555564256,
          144.88537516044843,
          263.26368383259364,
          251.69638522271023,
          255.49898582294657,
          55.59934254181465,
          296.7503380073173,
          255.7436707115384,
          221.7714875198629,
          232.41026012173472,
          262.827427757221,
          260.0071074603271,
          181.4977736167459,
          270.0368748972378,
          245.067412639416,
          265.2323807671722,
          272.80725036237857,
          271.4849227171868,
          242.90803307490205,
          251.1780889985992,
          269.415870751723,
          29.9989516502269,
          260.26929077832176,
          273.3216300026072,
          229.75619850715705,
          282.95545937913334,
          133.5730088168632,
          260.155321565878,
          249.65782543277956,
          272.66582977008056,
          268.3223544614758,
          6.5344340060123045,
          261.061254149864,
          264.144741046491,
          185.75432627538925,
          244.00695431205727,
          274.41424970346503,
          292.4709147110367,
          9.421881700972534,
          286.3867196718211,
          271.0002409782719,
          292.9248280634189,
          258.3684366984328,
          42.5502194203753,
          287.080057978011,
          310.3115653896352,
          237.1369634448444,
          25.528549203921216,
          232.02479729946535,
          296.16464095738183,
          268.64600348328804,
          48.13221909494817,
          269.1475039944462,
          -0.8963114977889006,
          266.18975123886514,
          9.219431307036883,
          303.22680437882605,
          263.3763990983296,
          275.3731687030049,
          241.23635696310492,
          282.67064443001965,
          12.291715900674546,
          214.7865535363052,
          220.0342209149277,
          308.36863890606014,
          263.9867157576407,
          247.9084871145357,
          247.35118747403263,
          304.0412950910045,
          257.19158318157275,
          245.7536270760625,
          265.4649856860804,
          258.11825357459105,
          277.68615588822774,
          264.877498073196,
          273.6806200739725,
          265.32030753955223,
          303.58178254205086,
          299.6081985448652,
          256.825503239082,
          261.03846332746303,
          49.08598632405415,
          251.19769051044884,
          277.1602028125305,
          241.19131261616013,
          217.88951380012756,
          300.8499897744667,
          140.26646945824012,
          113.4265419064729,
          244.87590214628523,
          11.83547867168187,
          215.24201824661736,
          268.2681251562781,
          267.40272387860887,
          182.81698807206485,
          13.909090611899714,
          247.6944122828625,
          288.695601461033,
          275.20558320517364,
          250.15876872863663,
          261.42741552898826,
          276.3573261817405,
          261.2677972130233,
          37.559293977877815,
          22.315485723910825,
          275.27639861393806,
          282.1211823424554,
          -28.8021732822195,
          32.48522449614438,
          41.59297155472879,
          33.41810719394374,
          259.72240162073115,
          254.79804697665537,
          227.1214667492551,
          252.5977953827264,
          275.06198919118856,
          42.51976908484477,
          246.72235552009067,
          241.34842751571108,
          286.7445806157787,
          135.32595129911738,
          273.87715236342774,
          213.8493285049581,
          207.4617385055756,
          266.3397734940718,
          205.29219304865177,
          237.746222009118,
          88.8552676513065,
          300.983377139748,
          205.3801053329341,
          239.12137309619757,
          230.67676674128356,
          268.2790166704141,
          297.23858876984315,
          286.0729950524108,
          264.3646373735276,
          236.75283685442616,
          317.5525197931355,
          269.34826567276014,
          247.91422883493195,
          298.41814261735965,
          314.0702115415904,
          222.81512319954348,
          231.3656338211985,
          241.1599057920596,
          264.35252096370874,
          229.32065108485187,
          276.66403475454933,
          219.09484590306,
          268.909097302539,
          283.29993591314616,
          251.769598595977,
          311.80639302106874,
          275.8275327129653,
          243.79915608686736,
          276.2993347790475,
          171.47550918784603,
          166.70933738984087,
          26.785079870091263,
          184.9123823838558,
          302.7495974709156,
          269.1558749744511,
          -4.019424076932779,
          244.46424228772955,
          262.6833111761939,
          282.40391553906943,
          234.90180470615172,
          -3.671832529661117,
          231.7524380434195,
          285.3814092546039,
          246.79967103884107,
          -17.473658582391224,
          271.54446873236407,
          248.59907369878397,
          264.19537553907776,
          261.90101825510067,
          264.9532783874146,
          17.025312763229508,
          -147.4123398788344,
          147.9365664076515,
          251.8365466421983,
          174.38997471145294,
          238.4212285035497,
          121.53764966420856,
          238.56062789118232,
          -28.216667908265222,
          6.552113944014024,
          124.34475134797282,
          -28.680670687185703,
          270.0304557463813,
          281.2046079555918,
          273.0415862311078,
          236.25059180531773,
          243.36224219341747,
          255.48976668835468,
          233.541253822383,
          211.18459342098885,
          229.53794700316516,
          249.75357624031065,
          287.3064577021012,
          306.75224720715994,
          243.94415625768403,
          253.24710801635405,
          256.415067352071,
          246.85495180012916,
          216.3107858791499,
          271.4132019338816,
          245.73229589979607,
          244.969560913608,
          35.29170761446472,
          197.5453538512287,
          251.81860193870696,
          267.85025369626544,
          231.83760234796887,
          237.2728243064092,
          269.5052074652878,
          260.63159641881464,
          240.2813634448144,
          247.03277379215731,
          259.459613246427,
          -93.55239906023029,
          262.6710593197655,
          253.04857144574692,
          242.04316559092425,
          266.6108049799724,
          -72.22565653258451,
          271.2416692579387,
          307.7128379157768,
          -28.76358244448393,
          295.75894911420147,
          289.9412277824864,
          295.4343345019796,
          -96.6657631567582,
          281.5003903485813,
          248.3867590174277,
          244.94433344420207,
          -36.59168548012126,
          291.4491414727062,
          290.1008740747425,
          257.2806761893455,
          220.56229316033577,
          203.3680730077021,
          44.37021933813509,
          265.40401321895075,
          13.25866493333173,
          274.84164006194555,
          261.1512077949632
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Reward and Last 100 Average Reward",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 2000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('DDQN_History.log', sep=';')\n",
    "plot_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andric\\anaconda3\\envs\\Reinforcement\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning:\n",
      "\n",
      "[WinError -2147417850] Cannot change thread mode after it is set\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 finished in 314 steps with reward 259.8623645173483.\n",
      "episode 2 finished in 288 steps with reward 257.05966515917913.\n",
      "episode 3 finished in 249 steps with reward 268.9174468096048.\n",
      "episode 4 finished in 204 steps with reward 237.14798515387955.\n",
      "episode 5 finished in 254 steps with reward 277.00182440486793.\n",
      "Episode reached the maximum number of steps. 450\n",
      "episode 6 finished in 450 steps with reward 151.74912420164515.\n",
      "episode 7 finished in 173 steps with reward 274.06344696161364.\n",
      "episode 8 finished in 208 steps with reward 266.6771424551007.\n",
      "episode 9 finished in 161 steps with reward 255.94001833874972.\n",
      "episode 10 finished in 275 steps with reward 264.34180405736043.\n",
      "Average reward: 251.27608220593493\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "filename = \"Models/DDQN/episode475.h5\"\n",
    "# 470-480 ep optimal\n",
    "trained_model = load_model(filename, custom_objects={\"masked_huber_loss\": MeanSquaredError()})\n",
    "\n",
    "evaluation_max_episodes = 10\n",
    "evaluation_max_steps = 450\n",
    "\n",
    "def episode_trigger(x):\n",
    "    return x % 1 == 0\n",
    "env = wrappers.RecordVideo(env, f'Videos/DDQN/', episode_trigger)\n",
    "\n",
    "def get_q_values(model, state):\n",
    "    input = state[np.newaxis, ...]\n",
    "    return model.predict(input)[0]\n",
    "\n",
    "def select_best_action(q_values):\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "rewards = []\n",
    "for episode in range(1, evaluation_max_episodes + 1):\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    step = 1\n",
    "    for step in range(1, evaluation_max_steps + 1):\n",
    "        env.render()\n",
    "        q_values = get_q_values(trained_model, state)\n",
    "        action = select_best_action(q_values)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if step == evaluation_max_steps:\n",
    "            print(f\"Episode reached the maximum number of steps. {evaluation_max_steps}\")\n",
    "            done = True\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"episode {episode} finished in {step} steps with reward {episode_reward}.\")\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im displaying episode 7 as it is my best episode based off rewards from the previous 10\n",
    "\n",
    "<video controls=\"true\" allowfullscreen=\"true\">\n",
    "  <source src=\"Videos\\DDQN\\rl-video-episode-7.mp4\">\n",
    "</video>\n",
    "\n",
    "You can see here that the Agent spent wasted very little resources when using the thrusters and it also landed the lander in very few steps. This shows that the agent has successfully learnt how to land the lander and also how to land it efficiently without wasting much time and also resources such as the thruster activations. The agent was able to fire the thrusters just enough so that the lander landed quickly but also not quickly enough to consider it as a crash. This is a great example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In conclusion, we explored two Reinforcement Learning Algorithms, DQN and its improved counterpart DDQN. We used DQN as our baseline model and after quite a bunch of parameter tuning, we got a average reward of around 220, which means what we have solved the problem according to the Lunar Lander doucmentation where they mentioned 'above 200 = solved'. After training our baseline model to solve the Lunar Lander problem, we trained DDQN, the baseline improved counterpart and after tuning it we managed to improve upon that average reward of 220 and managed to bring it up to 250-260 average reward, which indicates that our final model is able to learn and play the environment better than our baseline which was expected as DDQN is an improved counterpart to DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://wingedsheep.com/lunar-lander-dqn/\n",
    "- https://rubikscode.net/2021/07/20/introduction-to-double-q-learning/\n",
    "- https://arxiv.org/pdf/1509.06461.pdf\n",
    "- https://www.neuralnet.ai/\n",
    "- https://drawar.github.io/blog/2019/05/12/lunar-lander-dqn.html\n",
    "- https://youtu.be/p0rGjAgykOU"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef46218382cc8e7e0d821b47544cbee3314a6707750922047f71f70edd043db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Reinforcement')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
